{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EJDGpMcDWXQ"
      },
      "source": [
        "## Google Speech API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQgRX3XmCRTM"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets nltk evaluate tokenizers seqeval sequence-evaluate sentence-transformers rouge jiwer google-cloud-aiplatform google-cloud-aiplatform[all] google-cloud-speech librosa jiwer protobuf pydub  google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1726655525434,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "iZNQS1fKz_mY"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"cloud-llm-preview1\"  # @param {type:\"string\"}\n",
        "\n",
        "BUCKET_NAME = \"julien-us\" # @param {type:\"string\"}\n",
        "REGION = \"us-central1\" # @param {type:\"string\"}\n",
        "BQ_REGION = \"us\" # @param {type:\"string\"}\n",
        "table_id = \"julienmiquel_us.stt_speech2\" # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "executionInfo": {
          "elapsed": 2242,
          "status": "ok",
          "timestamp": 1726655530080,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "_r_TI3FguZu-"
      },
      "outputs": [],
      "source": [
        "# Where you stored your audio and your ground of truth\n",
        "wav_files = !gsutil ls gs://julien-us/stt_synthetic_tests_data/*.wav\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1726648425792,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "bbSJjHGVu5ZT",
        "outputId": "2018e5e8-ddda-4381-da3a-1fe825bd05d1"
      },
      "outputs": [],
      "source": [
        "text_files = [string.replace('.wav', '.txt') for string in wav_files]\n",
        "\n",
        "wav_text_arr = zip(wav_files, text_files)\n",
        "len(wav_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "executionInfo": {
          "elapsed": 283,
          "status": "ok",
          "timestamp": 1726655534685,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "kUCqhV8TwzSC"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "import re\n",
        "\n",
        "def split_gcs_uri(gcs_uri):\n",
        "  \"\"\"Splits a GCS URI into bucket name and blob path variables.\n",
        "\n",
        "  Args:\n",
        "    gcs_uri: The GCS URI to split.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the bucket name and blob path.\n",
        "  \"\"\"\n",
        "\n",
        "  match = re.match(r\"gs://([^/]+)/(.+)\", gcs_uri)\n",
        "  if match:\n",
        "    return match.groups()\n",
        "  else:\n",
        "    raise ValueError(\"Invalid GCS URI: {}\".format(gcs_uri))\n",
        "\n",
        "def write_file_to_gcs(gcs_bucket_name,  gcs_file_name, local_file_path, tags = None, verbose= False):\n",
        "    \"\"\"Writes a local file to GCS.\n",
        "\n",
        "    Args:\n",
        "    local_file_path: The path to the local file to write to GCS.\n",
        "    gcs_bucket_name: The name of the GCS bucket to write the file to.\n",
        "    gcs_file_name: The name of the GCS file to write the file to.\n",
        "\n",
        "    Returns:\n",
        "    The GCS file path.\n",
        "    \"\"\"\n",
        "    if verbose: print(f\"local_file_path = {local_file_path} - gcs_bucket_name = {gcs_bucket_name} - gcs_file_name = {gcs_file_name}\")\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = storage_client.bucket(gcs_bucket_name)\n",
        "    blob = bucket.blob(gcs_file_name)\n",
        "    if tags is not None:\n",
        "        blob.metadata = tags\n",
        "\n",
        "    if verbose: print(f\"upload_from_filename : local_file_path = {local_file_path}\")\n",
        "    blob.upload_from_filename(local_file_path, )\n",
        "\n",
        "    return blob\n",
        "\n",
        "\n",
        "def store_temp_video_from_gcs(bucket_name, file_name, localfile):\n",
        "    import tempfile\n",
        "    import os\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # try:\n",
        "    bytes_data = blob.download_as_bytes()\n",
        "\n",
        "    # Create a temporary file.\n",
        "    # tempDir = tempfile.gettempdir()\n",
        "    tempDir = os.getcwd()\n",
        "\n",
        "    temp_path = os.path.join(tempDir, localfile)\n",
        "    # f, temp_path = tempfile.mkstemp()\n",
        "    fp = open(temp_path, 'bw')\n",
        "    fp.write(bytes_data)\n",
        "    fp.seek(0)\n",
        "\n",
        "\n",
        "    return temp_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 949,
          "status": "ok",
          "timestamp": 1726655606560,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "KK3KitNU0wRq",
        "outputId": "33c7e280-471c-4157-aaf3-ff898faa7019"
      },
      "outputs": [],
      "source": [
        "from seq_eval import SeqEval\n",
        "import evaluate\n",
        "\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "evaluator = SeqEval()\n",
        "\n",
        "def evaluate_data(predictions, references, verbose=False, cleanup = False):\n",
        "    if cleanup:\n",
        "        references = [x for x in references if x!= '']\n",
        "        predictions = [x for x in predictions if x!= '']\n",
        "\n",
        "    if len(references)!= len(predictions):\n",
        "\n",
        "        min_arr = min(len(references), len(references))\n",
        "        print(f\"Reduce size to {min_arr}\")\n",
        "        predictions = predictions[0:min_arr]\n",
        "        references = references[0:min_arr]\n",
        "\n",
        "\n",
        "    scores = evaluator.evaluate(predictions, references, verbose=verbose)\n",
        "    if verbose: print(scores)\n",
        "\n",
        "    wer = wer_metric.compute(references=references, predictions=predictions)\n",
        "    wer = round(100 * wer, 2)\n",
        "    print(\"WER:\", wer ,end='\\n')\n",
        "    print(\"semantic_textual_similarity:\",scores['semantic_textual_similarity'],end='\\n')\n",
        "    return wer, scores['semantic_textual_similarity']\n",
        "\n",
        "evaluate_data([\"Hello\"], [\"Hello\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "executionInfo": {
          "elapsed": 279,
          "status": "ok",
          "timestamp": 1726655643641,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "_NffBK5rCNNv"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID, location=BQ_REGION)\n",
        "\n",
        "def save_results_df_bq(df, table_id, truncate = True):\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "\n",
        "    # Specify a (partial) schema. All columns are always written to the\n",
        "    # table. The schema is used to assist in data type definitions.\n",
        "\n",
        "    schema=[\n",
        "bigquery.SchemaField(\"input_file\",\"STRING\", mode=\"NULLABLE\"),\n",
        "bigquery.SchemaField(\"ground_truth\",\"STRING\", mode=\"NULLABLE\"),\n",
        "bigquery.SchemaField(\"model_name\",\"STRING\", mode=\"NULLABLE\"),\n",
        "bigquery.SchemaField(\"prompt\",\"STRING\", mode=\"NULLABLE\"),\n",
        "\n",
        "bigquery.SchemaField(\"wer\",\"FLOAT\", mode=\"NULLABLE\"),\n",
        "bigquery.SchemaField(\"semantic_textual_similarity\",\"FLOAT\", mode=\"NULLABLE\"),\n",
        "\n",
        "bigquery.SchemaField(\"generated_file\",\"STRING\", mode=\"NULLABLE\"),\n",
        "bigquery.SchemaField(\"generated_text\",\"STRING\", mode=\"NULLABLE\"),\n",
        "\n",
        "\n",
        "    ],\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
        "    # disposition it replaces the table with the loaded data.\n",
        "    #write_disposition=\"WRITE_TRUNCATE\",\n",
        "\n",
        "    )\n",
        "    if truncate:\n",
        "        print('truncate table: ' + table_id)\n",
        "        job_config.write_disposition=\"WRITE_TRUNCATE\"\n",
        "\n",
        "    job = client.load_table_from_dataframe(\n",
        "        df, table_id, job_config=job_config\n",
        "    )  # Make an API request.\n",
        "    job.result()  # Wait for the job to complete.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Sstf67OUNv"
      },
      "outputs": [],
      "source": [
        "text_files = [string.replace('.wav', '.txt') for string in wav_files]\n",
        "\n",
        "wav_text_arr = zip(wav_files, text_files)\n",
        "\n",
        "for wav_file, text_file in wav_text_arr:\n",
        "  print(wav_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUx0rUcyGRQ1"
      },
      "source": [
        "### One time init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 593,
          "status": "ok",
          "timestamp": 1726648427715,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "T4o_K1Q5DGpb",
        "outputId": "00172b74-d440-4dda-9550-3a21c46993ec"
      },
      "outputs": [],
      "source": [
        "from google.cloud.speech_v2 import SpeechClient\n",
        "from google.cloud.speech_v2.types import cloud_speech\n",
        "from google.api_core.client_options import ClientOptions\n",
        "\n",
        "def create_recognizer(language_code = \"fr-FR\", model_name = \"latest_long\", location = \"global\") -> cloud_speech.Recognizer:\n",
        "    # Instantiates a client\n",
        "    if location == \"global\":\n",
        "        client = SpeechClient()\n",
        "    else:\n",
        "        client = SpeechClient(\n",
        "            client_options=ClientOptions(api_endpoint=f\"{location}-speech.googleapis.com\")\n",
        "        )\n",
        "\n",
        "    recognizer_id = get_recognizer(language_code, model_name, location)\n",
        "\n",
        "\n",
        "    request = cloud_speech.CreateRecognizerRequest(\n",
        "        parent=f\"projects/{PROJECT_ID}/locations/{location}\",\n",
        "        recognizer_id=recognizer_id,\n",
        "        recognizer=cloud_speech.Recognizer(\n",
        "            default_recognition_config=cloud_speech.RecognitionConfig(\n",
        "                language_codes=[language_code], model=model_name\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "    try:\n",
        "        operation = client.create_recognizer(request=request)\n",
        "        recognizer = operation.result()\n",
        "\n",
        "        print(\"Created Recognizer:\", recognizer.name)\n",
        "    except Exception as e:\n",
        "        if e.__class__.__name__ == 'AlreadyExists':\n",
        "            print(\"Recognizer already exists:\", e)\n",
        "\n",
        "            return recognizer_id\n",
        "        else:\n",
        "            print(\"Failed to create recognizer:\", e)\n",
        "            return None\n",
        "    return recognizer_id\n",
        "\n",
        "def get_recognizer(language_code, model_name, location):\n",
        "    recognizer_id = f\"{model_name}-{language_code.lower()}-{location}\".replace(\"_\",\"-\")\n",
        "    return recognizer_id\n",
        "\n",
        "\n",
        "recognizer_chirp2 = create_recognizer(\"fr-FR\", \"chirp_2\", \"us-central1\")\n",
        "recognizer_long = create_recognizer(\"fr-FR\", \"long\", \"global\")\n",
        "recognizer_short = create_recognizer(\"fr-FR\", \"short\", \"global\")\n",
        "recognizer_latest_long = create_recognizer(\"fr-FR\", \"latest_long\", \"global\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_opTU8Bitu5P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.api_core import retry\n",
        "\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "def stt_chirp(audio_content, channels, frame_rate):\n",
        "    return stt_googleapi(audio_content, channels, frame_rate, model_name=\"chirp_2\")\n",
        "\n",
        "\n",
        "def stt_latest_long(audio_content, channels, frame_rate):\n",
        "    return stt_googleapi(audio_content, channels, frame_rate, model_name=\"latest_long\")\n",
        "\n",
        "@retry.Retry(timeout=3000.0)\n",
        "def stt_googleapi(audio_content, channels, frame_rate, model_name=\"default\", long_operation = False, verbose = False):\n",
        "    # Import the Speech-to-Text client library\n",
        "    from google.cloud import speech\n",
        "\n",
        "    # Instantiates a client\n",
        "    client = speech.SpeechClient()\n",
        "\n",
        "    # transcribe speech\n",
        "    audio = speech.RecognitionAudio(content=audio_content)\n",
        "\n",
        "    config = speech.RecognitionConfig(\n",
        "        #encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
        "        sample_rate_hertz=frame_rate,\n",
        "        language_code=\"fr-FR\",\n",
        "        model=model_name,\n",
        "        audio_channel_count=channels,\n",
        "        # enable_word_confidence=True,\n",
        "        # enable_word_time_offsets=True,\n",
        "        # enable_automatic_punctuation = True,\n",
        "        # enable_spoken_punctuation=True\n",
        "    )\n",
        "\n",
        "    if long_operation:\n",
        "        # Detects speech in the audio file\n",
        "        operation = client.long_running_recognize(config=config, audio=audio)\n",
        "\n",
        "        # print(\"Waiting for operation to complete...\")\n",
        "        response = operation.result(timeout=90)\n",
        "    else:\n",
        "        response = client.recognize(\n",
        "                    recognizer=f\"projects/{PROJECT_ID}/locations/global/recognizers/_\",\n",
        "            config=config, audio=audio)\n",
        "        \n",
        "    if verbose:\n",
        "        print(response)\n",
        "\n",
        "    results = []\n",
        "    for result in response.results:\n",
        "        if verbose:\n",
        "            print(\"Transcript: {}\".format(result.alternatives[0].transcript))\n",
        "        results.append(result.alternatives[0].transcript)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "@retry.Retry(timeout=3000.0)\n",
        "def stt_googleapi_v2(audio_content, channels, frame_rate, model_name=\"default\", verbose = False):\n",
        "    # Import the Speech-to-Text client library\n",
        "    from google.cloud.speech_v2 import SpeechClient\n",
        "    from google.cloud.speech_v2.types import cloud_speech\n",
        "\n",
        "    # Instantiates a client\n",
        "    client = SpeechClient()\n",
        "\n",
        "    # transcribe speech\n",
        "    config = cloud_speech.RecognitionConfig(\n",
        "        auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),\n",
        "        language_codes=[\"fr-FR\"],\n",
        "        model=model_name,\n",
        "    )\n",
        "\n",
        "    response = client.recognize(\n",
        "                recognizer=f\"projects/{PROJECT_ID}/locations/global/recognizers/_\",\n",
        "        config=config,\n",
        "                content=audio_content)\n",
        "    if verbose:\n",
        "        print(response)\n",
        "\n",
        "    results = []\n",
        "    for result in response.results:\n",
        "        if verbose:\n",
        "            print(\"Transcript: {}\".format(result.alternatives[0].transcript))\n",
        "        results.append(result.alternatives[0].transcript)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZH4FBxxQm-e"
      },
      "outputs": [],
      "source": [
        "def get_audio_sequence(file, min_silence_len=500):\n",
        "    from pydub import AudioSegment, silence\n",
        "\n",
        "    myaudio = AudioSegment.from_mp3(file)\n",
        "    dBFS=myaudio.dBFS\n",
        "\n",
        "    speak = silence.detect_nonsilent(myaudio, min_silence_len=min_silence_len, silence_thresh=dBFS-20, seek_step=10)\n",
        "    speak_sequences = speak\n",
        "\n",
        "    return speak_sequences\n",
        "\n",
        "def splitAudio(root_dir, file, start, stop, output_dir, verbose = False):\n",
        "    sound = AudioSegment.from_mp3(root_dir+file)\n",
        "    \n",
        "    sound = sound[start:stop]\n",
        "    if verbose:\n",
        "        print(80*\"__\")\n",
        "        print(f\"file = {file}\")\n",
        "        print(f\"duration_seconds = {sound.duration_seconds}\")\n",
        "        print(f\"sample_width = {sound.sample_width}\")\n",
        "        print(f\"channels = {sound.channels}\")\n",
        "        print(f\"frame_rate = {sound.frame_rate}\")\n",
        "\n",
        "    file_segment = output_dir+file+f\"-{start}-{stop}.wav\"\n",
        "\n",
        "    sound.export(file_segment, format=\"wav\")\n",
        "    return file_segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "executionInfo": {
          "elapsed": 284,
          "status": "ok",
          "timestamp": 1726655780983,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "pi_-t3NXIAPA"
      },
      "outputs": [],
      "source": [
        "from google.api_core.client_options import ClientOptions\n",
        "\n",
        "def stt_api_v2(content, recognizer, channels , frame_rate,model_name, language_code = \"fr-FR\", location = \"us-central1\"):\n",
        "\n",
        "\n",
        "    if \"global\" == location:\n",
        "        client = SpeechClient()\n",
        "    else:\n",
        "        client = SpeechClient(\n",
        "            client_options=ClientOptions(api_endpoint=f\"{location}-speech.googleapis.com\")\n",
        "        )\n",
        "\n",
        "    short_audio_config = cloud_speech.RecognitionConfig(\n",
        "        features=cloud_speech.RecognitionFeatures(\n",
        "            enable_automatic_punctuation=True, enable_word_time_offsets=True\n",
        "        ),\n",
        "        auto_decoding_config={},\n",
        "        #explicit_decoding_config = cloud_speech.ExplicitDecodingConfig(            sample_rate_hertz=frame_rate,            #audio_channel_count = channels             ),\n",
        "        language_codes = [language_code],\n",
        "        model = model_name\n",
        "        # audio_channel_count = channels,\n",
        "        # sample_rate_hertz  = frame_rate\n",
        "    )\n",
        "\n",
        "    short_audio_request = cloud_speech.RecognizeRequest(\n",
        "        #recognizer=recognizer,\n",
        "        recognizer=f\"projects/{PROJECT_ID}/locations/{location}/recognizers/_\",\n",
        "        config=short_audio_config, content=content,\n",
        "    )\n",
        "    results = []\n",
        "    try:\n",
        "        response = client.recognize(request=short_audio_request)\n",
        "\n",
        "\n",
        "        for result in response.results:\n",
        "            print(f\"Transcript: {result.alternatives[0].transcript}\")\n",
        "            results.append (result.alternatives[0].transcript+\" \")\n",
        "    except Exception as e:\n",
        "        print(\"Exception: \",e)\n",
        "    return \"\".join(results)\n",
        "\n",
        "\n",
        "\n",
        "def stt_api_v2_chirp2(content, channels , frame_rate):\n",
        "    return stt_api_v2(content, recognizer_chirp2, channels , frame_rate,\"chirp_2\", language_code = \"fr-FR\", location = \"us-central1\")\n",
        "\n",
        "def stt_api_v2_long(content, channels , frame_rate):\n",
        "    return stt_api_v2(content, recognizer_long, channels , frame_rate, \"long\", \"fr-FR\" , location = \"global\")\n",
        "\n",
        "def stt_api_v2_short(content, channels , frame_rate):\n",
        "    return stt_api_v2(content, recognizer_short, channels , frame_rate, \"short\",\"fr-FR\" , location = \"global\")\n",
        "\n",
        "def stt_api_v2_latest_long(content, channels , frame_rate):\n",
        "    return stt_api_v2(content, recognizer_short, channels , frame_rate, \"latest_long\",\"fr-FR\" , location = \"global\")\n",
        "\n",
        "\n",
        "\n",
        "models_v2 = { \"Speech_split:chirp_2\":stt_api_v2_chirp2,\n",
        "              \"Speech:long\": stt_api_v2_long,\n",
        "              \"#Speech:short\": stt_api_v2_short,\n",
        "              \"Speech:latest_long\": stt_api_v2_latest_long\n",
        "             }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1726655782332,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "5eOuepFAGpM2"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pydub\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "def process_file(file_name, _stt, verbose=False):\n",
        "\n",
        "    sound = AudioSegment.from_mp3(file_name )\n",
        "    if verbose:\n",
        "        print(f\"duration_seconds = {sound.duration_seconds}\")\n",
        "        print(f\"sample_width = {sound.sample_width}\")\n",
        "        print(f\"channels = {sound.channels}\")\n",
        "        print(f\"frame_rate = {sound.frame_rate}\")\n",
        "        sound.get_sample_slice()\n",
        "        \n",
        "    finish = False\n",
        "    INCREMENT = 59*1000\n",
        "    start = 0\n",
        "    stop = INCREMENT\n",
        "\n",
        "    results = []\n",
        "\n",
        "    while(finish == False):\n",
        "        stop = start+(INCREMENT)\n",
        "        buffer = io.BytesIO()\n",
        "        sound[start:stop].export(buffer, format=\"wav\" )\n",
        "        batch_result = _stt(buffer.read(), channels = sound.channels, frame_rate=sound.frame_rate)\n",
        "        print(f\"start = {start} - stop = {stop}\")\n",
        "\n",
        "        \n",
        "        results.extend(\" \"+batch_result)\n",
        "        if stop > (sound.duration_seconds*1000):\n",
        "            finish = True\n",
        "        else:\n",
        "            start += INCREMENT\n",
        "\n",
        "\n",
        "    return \"\".join(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "executionInfo": {
          "elapsed": 253,
          "status": "ok",
          "timestamp": 1726655784560,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "PB6MT44eIyRW"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pydub\n",
        "from pydub import AudioSegment\n",
        "\n",
        "root_dir = '.'\n",
        "output_dir = '.'\n",
        "\n",
        "def process_fileV2(file_name, _stt, verbose=False):\n",
        "\n",
        "    sound = AudioSegment.from_mp3(file_name )\n",
        "    if verbose:\n",
        "      print(f\"duration_seconds = {sound.duration_seconds}\")\n",
        "      print(f\"sample_width = {sound.sample_width}\")\n",
        "      print(f\"channels = {sound.channels}\")\n",
        "      print(f\"frame_rate = {sound.frame_rate}\")\n",
        "\n",
        "    results = []\n",
        "    min_silence_len = 600\n",
        "    speak_sequences = get_audio_sequence(file_name, min_silence_len)\n",
        "\n",
        "    # filter speak_sequences when stop - start are more than 59 secondes\n",
        "    speak_sequences_too_big = [(start, stop) for start, stop in speak_sequences if stop - start > 59000]\n",
        "\n",
        "    while(len(speak_sequences_too_big) > 0 and min_silence_len !=100 ):\n",
        "\n",
        "      min_silence_len =     min (min_silence_len-100, 100)\n",
        "      speak_sequences = get_audio_sequence(file_name, min_silence_len)\n",
        "      # filter speak_sequences when stop - start are more than 59 secondes\n",
        "      speak_sequences_too_big = [(start, stop) for start, stop in speak_sequences if stop - start > 59000]\n",
        "      print(\"Sequence more than 59s : \"+len(speak_sequences_too_big))\n",
        "      print(f\"min_silence_len = {min_silence_len}\")\n",
        "\n",
        "    # filter speak_sequences when stop - start are less than 1,5 secondes\n",
        "    #speak_sequences_filtered = [(start, stop) for start, stop in speak_sequences if stop - start >= 1500]\n",
        "\n",
        "\n",
        "    for (start, stop) in speak_sequences:\n",
        "\n",
        "        buffer = io.BytesIO()\n",
        "\n",
        "        sound[start:stop].export(buffer, format=\"wav\" )\n",
        "        batch_result = _stt(buffer.read(), channels = sound.channels, frame_rate=sound.frame_rate)\n",
        "        print(f\"start = {start} - stop = {stop}\")\n",
        "\n",
        "        results.extend(batch_result+\" \")\n",
        "\n",
        "\n",
        "    return \"\".join(results)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 164124,
          "status": "ok",
          "timestamp": 1726657039282,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "46m6unH9CWC2",
        "outputId": "9b3456c5-3266-4b7c-c638-f48e73ba2d47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "for model_name in models_v2:\n",
        "  print(f\"Model: {model_name}\")\n",
        "  text_files = [string.replace('.wav', '.txt') for string in wav_files]\n",
        "  wav_text_arr = zip(wav_files, text_files)\n",
        "\n",
        "  i = 0\n",
        "  for wav_file, text_file in wav_text_arr:\n",
        "    print(f\"Processing {i}\")\n",
        "    bucket, file_wav = split_gcs_uri(wav_file)\n",
        "    bucket, file_txt = split_gcs_uri(text_file)\n",
        "    local_file = f\"{i}-temp.wav\"\n",
        "    local_file_txt = f\"{i}-ground-truth.txt\"\n",
        "    store_temp_video_from_gcs(bucket, file_wav, local_file)\n",
        "    store_temp_video_from_gcs(bucket, file_txt, local_file_txt)\n",
        "\n",
        "    result = process_file(local_file,models_v2[model_name])\n",
        "\n",
        "    gemini_file = f\"{i}-speech_{model_name}_result.json\"\n",
        "    with open(gemini_file, \"w\", encoding=\"UTF8\") as f:\n",
        "        f.write(result)\n",
        "\n",
        "    tags = { \"model_name\": model_name,\n",
        "            \"file\": file_wav,\n",
        "            \"ground-truth\": file_txt,\n",
        "            }\n",
        "    write_file_to_gcs(bucket,  text_file.replace(\"stt_synthetic_tests_data\", \"stt_synthetic_results\").replace(\".txt\",\"\") + f\"-gemini-{model_name}.txt\",\n",
        "                      gemini_file, tags )\n",
        "\n",
        "\n",
        "    with open(local_file_txt, 'r') as f:\n",
        "      ground_truth = f.read()\n",
        "    ground_truth = ground_truth.replace(\"\\n\", \" \")\n",
        "\n",
        "    wer, semantic_textual_similarity = evaluate_data([result], [ground_truth])\n",
        "    print(f\"Results:{wav_file}, WER: {wer}, semantic_textual_similarity: {semantic_textual_similarity}\")\n",
        "\n",
        "    data = {\n",
        "      \"input_file\": wav_file,\n",
        "      \"ground_truth\": ground_truth,\n",
        "      \"model_name\": model_name,\n",
        "      \"prompt\": \"\",\n",
        "      \"wer\": wer,\n",
        "      \"semantic_textual_similarity\": semantic_textual_similarity,\n",
        "      \"generated_file\": gemini_file,\n",
        "      \"generated_text\": result\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame( data = [data], columns = [\"input_file\",\"ground_truth\", \"wer\", \"semantic_textual_similarity\",\"generated_file\",\"generated_text\" , \"model_name\", \"prompt\"])\n",
        "\n",
        "    save_results_df_bq(df, table_id, truncate=False)\n",
        "\n",
        "    i += 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 258,
          "status": "ok",
          "timestamp": 1726663580629,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "UxptFtIhnNbX",
        "outputId": "b81c4d24-badc-4168-8b72-a086df13b278"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "STT google speech benchmark.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
