# -*- coding: utf-8 -*-
"""go/reproduce-voice-issue-multi-speaker

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cmqsp8D8B2SEf_qIOz6Oxd8Vi8biHKMt

Use this template to reproduce issues with multi speaker voice output from Text-to-Speech API models (Standard, Wavenet, Neural2, Studio, Chirp 3 HD) and Vertex AI / Gemini API models using Python SDK.

For single speaker issues use [go/reproduce-voice-issue](go/reproduce-voice-issue).

When using Vertex AI / Gemini API enable Sherlog. The Sherlog links will then be printed during Audio generation.

A copy of this notebook can be shared with customers.

This notebook will synthesize audio from the input provided. It will then transcribe it with the `chirp_3` STT model to check for synthesis errors. Finally the results are analyzed with Gemini to check against the `ISSUE_DESCRIPTION` (if provided) and to identify if there are any issues between input and transcript.

The result will be visualized by an audio player with synchronized text playback. Missing / incorrect words will be shown in $\color{IndianRed}{\text{red}}$, additional words will be shown in $\color{CornflowerBlue}{\text{blue}}$.

# IMPORTANT

Review the [Google Cloud Text-to-Speech documentation](https://docs.cloud.google.com/text-to-speech/docs) especially the [Gemini TTS](https://docs.cloud.google.com/text-to-speech/docs/gemini-tts) or [Chirp 3: HD Voices](https://docs.cloud.google.com/text-to-speech/docs/chirp3-hd) pages and use MARKUP as input type if you want to use speech markup.

Create a copy of this notebook, share it with project-nexus-restricted-group-external@google.com and submit via [go/model-quality-issue](http://go/model-quality-issue)
"""

# @title Project Setup

GCP_PROJECT_ID="nexus-tiger-team-sandbox" # @param {type:"string"}
LOCATION="global" # @param ["europe-central2","europe-north1","europe-southwest1","europe-west1","europe-west2","europe-west3","europe-west4","europe-west6","europe-west8","europe-west9","global","northamerica-northeast1","southamerica-east1","us-central1","us-east1","us-east4","us-east5","us-south1","us-west1","us-west4"] {allow-input: true}
OUTPUT_BUCKET="nexus-tiger-team-sandbox-evals" # @param {type:"string"}
GEMINI_API_KEY="" # @param {type:"string", placeholder:"Only required if the Gemini API should be used instead of Vertex AI"}
DEBUG_LOGGING=False # @param {type:"boolean"}

# @markdown If the issue can be reproduced, this slider allows to generate a dataset of texts with similar issues.
REPRODUCTION_DATASET_SIZE = 0 # @param {type:"slider", min:0, max: 100, step: 1}

VOICE_CONFIGS=[]

# Verifications
if not GCP_PROJECT_ID:
  raise ValueError("GCP_PROJECT_ID is mandatory")
if not LOCATION:
  raise ValueError("LOCATION is mandatory")
if not OUTPUT_BUCKET:
  raise ValueError("OUTPUT_BUCKET is mandatory")

# @title Evaluation Configuration

# @markdown Describe in detail the issue you are encountering
ISSUE_DESCRIPTION="The audio contains static noise" # @param {type:"string", placeholder:"Optional description of the issue to be analyzed"}

#@markdown STT model to be used to transcribe the synthesized TTS audio
STT_TRANSCRIPTION_MODEL="chirp_3" # @param ["chirp_3","chirp_2","long","short","telephony"] {allow-input: true}

#@markdown Gemini model to use for evaluation
GEMINI_EVALUATION_MODEL="gemini-3-flash-preview" # @param ["gemini-2.5-flash","gemini-2.5-pro","gemini-3-pro-preview","gemini-3-flash-preview"] {allow-input: true}

# @title Multi speaker synthesis configuration

# @markdown **Remove if using single speaker configuration! Use this configuration template for multi-speaker TTS. You can copy and paste this cell if you want to compare different configurations.**

# @markdown Choose the input source type, SSML is not supported with Chirp 3 HD and Gemini voices
INPUT_SOURCE="MULTI_SPEAKER_MARKUP" # @param ["TEXT","SSML","MARKUP","MULTI_SPEAKER_MARKUP"]

# @markdown TEXT must be in the following text format (Speaker Alias: Speaker Text followed by newline). You can use 2 consecutive newlines to force a new input (required for TTS API):
# @markdown ```
# @markdown Joe: How's it going today Jane?\nJane: Not too bad! Honestly, I'm just blown away by this new Gemini Text-to-Speech model.\nJoe: Right?! This is a total game changer! And do you know what I just discovered?\nJane: What?\nJoe: You can use the multi-speaker configuration to create a conversation, just like us!\nJane: Just like us!
# @markdown ```

# @markdown Provide the input text to be synthesized
TEXT="Joe: How's it going today Jane?\nJane: Not too bad! Honestly, I'm just blown away by this new Gemini Text-to-Speech model.\nJoe: Right?! This is a total game changer! And do you know what I just discovered?\nJane: What?\nJoe: You can use the multi-speaker configuration to create a conversation, just like us!\nJane: Just like us!" # @param {type:"string",placeholder:"Text/SSML/Markup to be synthesized"}

# @markdown The TTS API allows only 4000 bytes of input text. Auto will split the text by 2 consecutive newlines in the TEXT or automatically select splits to create equally large chunks of multiple turns. Disabled will not split.
SPLIT_BY_TURNS="auto" # @param ["auto","disabled"]

# @markdown (Optional) Provide a style prompt - only supported for Gemini voices
PROMPT="Create a lively discussion between Joe and Jane" # @param {type:"string",placeholder:"Optional style prompt, only for Gemini TTS"}

# @markdown Change the output encoding - leave it as LINEAR16 unless you know what you are doing
OUTPUT_ENCODING="LINEAR16" # @param ["LINEAR16","ALAW","MULAW","MP3","OGG_OPUS","PCM"]

# @markdown Change the output sample rate - leave it as 24000 unless you know what you are doing
OUTPUT_SAMPLE_RATE="24000" # @param ["8000","16000","24000","48000"]

# @markdown Count of synthesized voice audio outputs to create (test with 1, use 10 to 100 for final evaluation)
COUNT = 10 # @param {type:"slider", min:1, max: 100}

# @markdown ## Voice Configuration

SERVICE="Vertex AI" # @param ["TTS","Vertex AI","Gemini API"]
API="generateContent" # @param ["synthesize", "synthesizeLongAudio", "streaming","generateContent","streamingGenerateContent"]
SPEAKING_RATE=1 # @param {type:"slider", min:0.25, max: 2.0, step: 0.01}
PITCH=0 # @param {type:"slider", min:-20.0, max:20.0, step:0.1}
VOLUME_GAIN_DB=0 # @param {type:"slider", min:-96.0, max:16.0, step:0.1}
EFFECTS_PROFILE_ID="" # @param ["","wearable-class-device","handset-class-device","headphone-class-device","small-bluetooth-speaker-class-device","medium-bluetooth-speaker-class-device","large-home-entertainment-class-device","large-automotive-class-device","telephony-class-application"]
TEMPERATURE=1 # @param {type:"slider", min:0.0, max: 2.0, step: 0.1}
VOICE_FAMILY="gemini-2.5-pro-tts" # @param ["gemini-2.5-flash-lite-preview-tts","gemini-2.5-flash-tts","gemini-2.5-pro-tts","Standard","Wavenet","Neural2","Polyglot","Studio","News","Chirp3-HD"] {allow-input: true}
LOCALE="de-DE" # @param ["af-ZA","am-ET","ar-XA","bg-BG","bn-IN","ca-ES","cmn-CN","cmn-TW","cs-CZ","da-DK","de-DE","el-GR","en-AU","en-GB","en-IN","en-US","es-ES","es-US","et-EE","eu-ES","fi-FI","fil-PH","fr-CA","fr-FR","gl-ES","gu-IN","he-IL","hi-IN","hr-HR","hu-HU","id-ID","is-IS","it-IT","ja-JP","kn-IN","ko-KR","lt-LT","lv-LV","ml-IN","mr-IN","ms-MY","nb-NO","nl-BE","nl-NL","pa-IN","pl-PL","pt-BR","pt-PT","ro-RO","ru-RU","sk-SK","sl-SI","sr-RS","sv-SE","sw-KE","ta-IN","te-IN","th-TH","tr-TR","uk-UA","ur-IN","vi-VN","yue-HK"] {allow-input: true}

# @markdown Voice configuration for Speaker 1

SPEAKER1_ALIAS="Joe" # @param {type:"string", placeholder: "Speaker 1 Alias only for Multi-Speaker config"}
SPEAKER1_VOICE_NAME="Achird" # @param ["Achernar","Achird","Algenib","Algieba","Alnilam","Aoede","Autonoe","Callirrhoe","Charon","Despina","Enceladus","Erinome","Fenrir","Gacrux","Iapetus","Kore","Laomedeia","Leda","Orus","Pulcherrima","Puck","Rasalgethi","Sadachbia","Sadaltager","Schedar","Sulafat","Umbriel","Vindemiatrix","Zephyr","Zubenelgenubi","A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z","1"] {allow-input: true}

# @markdown Voice configuration for Speaker 2

SPEAKER2_ALIAS="Jane" # @param {type:"string", placeholder: "Speaker 2 Alias only for Multi-Speaker config"}
SPEAKER2_VOICE_NAME="Achernar" # @param ["","Achernar","Achird","Algenib","Algieba","Alnilam","Aoede","Autonoe","Callirrhoe","Charon","Despina","Enceladus","Erinome","Fenrir","Gacrux","Iapetus","Kore","Laomedeia","Leda","Orus","Pulcherrima","Puck","Rasalgethi","Sadachbia","Sadaltager","Schedar","Sulafat","Umbriel","Vindemiatrix","Zephyr","Zubenelgenubi","A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z","1"] {allow-input: true}

# @markdown Voice configuration for Speaker 3 (optional)
SPEAKER3_ALIAS="" # @param {type:"string", placeholder: "Speaker 3 Alias only for Multi-Speaker config"}
SPEAKER3_VOICE_NAME="" # @param ["","Achernar","Achird","Algenib","Algieba","Alnilam","Aoede","Autonoe","Callirrhoe","Charon","Despina","Enceladus","Erinome","Fenrir","Gacrux","Iapetus","Kore","Laomedeia","Leda","Orus","Pulcherrima","Puck","Rasalgethi","Sadachbia","Sadaltager","Schedar","Sulafat","Umbriel","Vindemiatrix","Zephyr","Zubenelgenubi","A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z","1"] {allow-input: true}

VOICE_CONFIGS.append({"INPUT_SOURCE":INPUT_SOURCE,"TEXT":TEXT,"SPLIT_BY_TURNS":SPLIT_BY_TURNS,"PROMPT":PROMPT,"OUTPUT_ENCODING":OUTPUT_ENCODING,"OUTPUT_SAMPLE_RATE":OUTPUT_SAMPLE_RATE,"COUNT":COUNT,"SERVICE":SERVICE,"API":API,"VOICE_FAMILY":VOICE_FAMILY,"SPEAKER1_VOICE_NAME":SPEAKER1_VOICE_NAME,"SPEAKER1_ALIAS":SPEAKER1_ALIAS,"SPEAKER2_VOICE_NAME":SPEAKER2_VOICE_NAME,"SPEAKER2_ALIAS":SPEAKER2_ALIAS,"SPEAKER3_VOICE_NAME":SPEAKER3_VOICE_NAME,"SPEAKER3_ALIAS":SPEAKER3_ALIAS,"LOCALE":LOCALE,"TEMPERATURE":TEMPERATURE})

# @title Multi speaker synthesis configuration

# @markdown **Remove if using single speaker configuration! Use this configuration template for multi-speaker TTS. You can copy and paste this cell if you want to compare different configurations.**

# @markdown Choose the input source type, SSML is not supported with Chirp 3 HD and Gemini voices
INPUT_SOURCE="MULTI_SPEAKER_MARKUP" # @param ["TEXT","SSML","MARKUP","MULTI_SPEAKER_MARKUP"]

# @markdown TEXT must be in the following text format (Speaker Alias: Speaker Text followed by newline). You can use 2 consecutive newlines to force a new input (required for TTS API):
# @markdown ```
# @markdown Joe: How's it going today Jane?\nJane: Not too bad! Honestly, I'm just blown away by this new Gemini Text-to-Speech model.\nJoe: Right?! This is a total game changer! And do you know what I just discovered?\nJane: What?\nJoe: You can use the multi-speaker configuration to create a conversation, just like us!\nJane: Just like us!
# @markdown ```

# @markdown Provide the input text to be synthesized
TEXT="Joe: How's it going today Jane?\nJane: Not too bad! Honestly, I'm just blown away by this new Gemini Text-to-Speech model.\nJoe: Right?! This is a total game changer! And do you know what I just discovered?\nJane: What?\nJoe: You can use the multi-speaker configuration to create a conversation, just like us!\nJane: Just like us!" # @param {type:"string",placeholder:"Text/SSML/Markup to be synthesized"}

# @markdown The TTS API allows only 4000 bytes of input text. Auto will split the text by 2 consecutive newlines in the TEXT or automatically select splits to create equally large chunks of multiple turns. Disabled will not split.
SPLIT_BY_TURNS="auto" # @param ["auto","disabled"]

# @markdown (Optional) Provide a style prompt - only supported for Gemini voices
PROMPT="Create a lively discussion between Joe and Jane" # @param {type:"string",placeholder:"Optional style prompt, only for Gemini TTS"}

# @markdown Change the output encoding - leave it as LINEAR16 unless you know what you are doing
OUTPUT_ENCODING="LINEAR16" # @param ["LINEAR16","ALAW","MULAW","MP3","OGG_OPUS","PCM"]

# @markdown Change the output sample rate - leave it as 24000 unless you know what you are doing
OUTPUT_SAMPLE_RATE="24000" # @param ["8000","16000","24000","48000"]

# @markdown Count of synthesized voice audio outputs to create (test with 1, use 10 to 100 for final evaluation)
COUNT = 2 # @param {type:"slider", min:1, max: 100}

# @markdown ## Voice Configuration

SERVICE="Vertex AI" # @param ["TTS","Vertex AI","Gemini API"]
API="generateContent" # @param ["synthesize", "synthesizeLongAudio", "streaming","generateContent","streamingGenerateContent"]
SPEAKING_RATE=1 # @param {type:"slider", min:0.25, max: 2.0, step: 0.01}
PITCH=0 # @param {type:"slider", min:-20.0, max:20.0, step:0.1}
VOLUME_GAIN_DB=0 # @param {type:"slider", min:-96.0, max:16.0, step:0.1}
EFFECTS_PROFILE_ID="" # @param ["","wearable-class-device","handset-class-device","headphone-class-device","small-bluetooth-speaker-class-device","medium-bluetooth-speaker-class-device","large-home-entertainment-class-device","large-automotive-class-device","telephony-class-application"]
TEMPERATURE=0.5 # @param {type:"slider", min:0.0, max: 2.0, step: 0.1}
VOICE_FAMILY="gemini-2.5-pro-tts" # @param ["gemini-2.5-flash-lite-preview-tts","gemini-2.5-flash-tts","gemini-2.5-pro-tts","Standard","Wavenet","Neural2","Polyglot","Studio","News","Chirp3-HD"] {allow-input: true}
LOCALE="de-DE" # @param ["af-ZA","am-ET","ar-XA","bg-BG","bn-IN","ca-ES","cmn-CN","cmn-TW","cs-CZ","da-DK","de-DE","el-GR","en-AU","en-GB","en-IN","en-US","es-ES","es-US","et-EE","eu-ES","fi-FI","fil-PH","fr-CA","fr-FR","gl-ES","gu-IN","he-IL","hi-IN","hr-HR","hu-HU","id-ID","is-IS","it-IT","ja-JP","kn-IN","ko-KR","lt-LT","lv-LV","ml-IN","mr-IN","ms-MY","nb-NO","nl-BE","nl-NL","pa-IN","pl-PL","pt-BR","pt-PT","ro-RO","ru-RU","sk-SK","sl-SI","sr-RS","sv-SE","sw-KE","ta-IN","te-IN","th-TH","tr-TR","uk-UA","ur-IN","vi-VN","yue-HK"] {allow-input: true}

# @markdown Voice configuration for Speaker 1

SPEAKER1_ALIAS="Joe" # @param {type:"string", placeholder: "Speaker 1 Alias only for Multi-Speaker config"}
SPEAKER1_VOICE_NAME="Achird" # @param ["Achernar","Achird","Algenib","Algieba","Alnilam","Aoede","Autonoe","Callirrhoe","Charon","Despina","Enceladus","Erinome","Fenrir","Gacrux","Iapetus","Kore","Laomedeia","Leda","Orus","Pulcherrima","Puck","Rasalgethi","Sadachbia","Sadaltager","Schedar","Sulafat","Umbriel","Vindemiatrix","Zephyr","Zubenelgenubi","A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z","1"] {allow-input: true}

# @markdown Voice configuration for Speaker 2

SPEAKER2_ALIAS="Jane" # @param {type:"string", placeholder: "Speaker 2 Alias only for Multi-Speaker config"}
SPEAKER2_VOICE_NAME="Achernar" # @param ["","Achernar","Achird","Algenib","Algieba","Alnilam","Aoede","Autonoe","Callirrhoe","Charon","Despina","Enceladus","Erinome","Fenrir","Gacrux","Iapetus","Kore","Laomedeia","Leda","Orus","Pulcherrima","Puck","Rasalgethi","Sadachbia","Sadaltager","Schedar","Sulafat","Umbriel","Vindemiatrix","Zephyr","Zubenelgenubi","A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z","1"] {allow-input: true}

# @markdown Voice configuration for Speaker 3 (optional)
SPEAKER3_ALIAS="" # @param {type:"string", placeholder: "Speaker 3 Alias only for Multi-Speaker config"}
SPEAKER3_VOICE_NAME="" # @param ["","Achernar","Achird","Algenib","Algieba","Alnilam","Aoede","Autonoe","Callirrhoe","Charon","Despina","Enceladus","Erinome","Fenrir","Gacrux","Iapetus","Kore","Laomedeia","Leda","Orus","Pulcherrima","Puck","Rasalgethi","Sadachbia","Sadaltager","Schedar","Sulafat","Umbriel","Vindemiatrix","Zephyr","Zubenelgenubi","A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z","1"] {allow-input: true}

VOICE_CONFIGS.append({"INPUT_SOURCE":INPUT_SOURCE,"TEXT":TEXT,"SPLIT_BY_TURNS":SPLIT_BY_TURNS,"PROMPT":PROMPT,"OUTPUT_ENCODING":OUTPUT_ENCODING,"OUTPUT_SAMPLE_RATE":OUTPUT_SAMPLE_RATE,"COUNT":COUNT,"SERVICE":SERVICE,"API":API,"VOICE_FAMILY":VOICE_FAMILY,"SPEAKER1_VOICE_NAME":SPEAKER1_VOICE_NAME,"SPEAKER1_ALIAS":SPEAKER1_ALIAS,"SPEAKER2_VOICE_NAME":SPEAKER2_VOICE_NAME,"SPEAKER2_ALIAS":SPEAKER2_ALIAS,"SPEAKER3_VOICE_NAME":SPEAKER3_VOICE_NAME,"SPEAKER3_ALIAS":SPEAKER3_ALIAS,"LOCALE":LOCALE,"TEMPERATURE":TEMPERATURE})

# @title Setup

!pip install --quiet --upgrade google-genai google-cloud-texttospeech google-cloud-speech tqdm jiwer praat-parselmouth librosa

import google.auth
from google.auth import downscoped
from google.api_core import exceptions
from google.api_core.client_options import ClientOptions

from google import genai
from google.genai import types

from google.cloud import texttospeech
from google.cloud import speech_v2
from google.cloud.speech_v2.types import cloud_speech

from google.cloud import storage

from google.colab import auth

from IPython.display import Image, display, Audio, HTML, Markdown, JSON
import jiwer
import librosa
import numpy as np
import parselmouth
from parselmouth.praat import call
from pydantic import BaseModel
from tqdm.auto import tqdm

import base64
import copy
import datetime
import difflib
import io
import json
import logging
import os
import re
import sys
import tempfile
import uuid
import wave

# Configure logging
class OrangeWarningFormatter(logging.Formatter):
    ORANGE = "\033[38;2;255;165;0m"
    RESET = "\033[0m"
    def format(self, record):
        if record.levelno == logging.WARNING:
            return f"{self.ORANGE}{super().format(record)}{self.RESET}"
        return super().format(record)

logging.basicConfig()
logger = logging.getLogger()

# Update handlers
for handler in logger.handlers:
    handler.setFormatter(OrangeWarningFormatter('%(levelname)s:%(name)s:%(message)s'))

if 'DEBUG_LOGGING' in globals() and DEBUG_LOGGING:
  logger.setLevel(logging.DEBUG)
else:
  logger.setLevel(logging.INFO)

# Suppress debug logging for libraries
logging.getLogger("google_genai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("numba").setLevel(logging.WARNING)

logger.debug("Authenticating user")

auth.authenticate_user()
credentials,_=google.auth.default()
request = google.auth.transport.requests.Request()
credentials.refresh(request)
token=credentials.token
sherlog_header = {'headers': {'Authorization': f'Bearer {token}'}}

logger.debug("Initializing SDK clients")

# define TTS clients
tts_client_options = ClientOptions(quota_project_id=GCP_PROJECT_ID)
if LOCATION != "global":
  tts_client_options.api_endpoint = f"{LOCATION}-texttospeech.googleapis.com"
tts_client = texttospeech.TextToSpeechClient(client_options=tts_client_options)
# Use dedicated client for Long Audio
tts_long_audio_client = texttospeech.TextToSpeechLongAudioSynthesizeClient(client_options=tts_client_options)

eval_genai_client = genai.Client(
    vertexai=True,
    project=GCP_PROJECT_ID,
    location=LOCATION,
)

# @title Configuration Validation

WARN_COLOR = "\033[38;2;255;165;0m"
RESET_COLOR = "\033[0m"

if not VOICE_CONFIGS:
  print(f"{WARN_COLOR}Warning: VOICE_CONFIGS is empty. Please run a configuration cell first.{RESET_COLOR}")

if 'ISSUE_DESCRIPTION' not in globals() or not ISSUE_DESCRIPTION:
  print(f"{WARN_COLOR}Warning: ISSUE_DESCRIPTION is missing{RESET_COLOR}")

CHIRP_GEMINI_NAMES = ["Achernar", "Achird", "Algenib", "Algieba", "Alnilam", "Aoede", "Autonoe", "Callirrhoe", "Charon", "Despina", "Enceladus", "Erinome", "Fenrir", "Gacrux", "Iapetus", "Kore", "Laomedeia", "Leda", "Orus", "Pulcherrima", "Puck", "Rasalgethi", "Sadachbia", "Sadaltager", "Schedar", "Sulafat", "Umbriel", "Vindemiatrix", "Zephyr", "Zubenelgenubi"]
STANDARD_NAMES = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
POLYGLOT_NAMES = ["1"]

for idx, vc in enumerate(VOICE_CONFIGS):
  text = vc.get("TEXT")
  input_source = vc.get("INPUT_SOURCE")
  service = vc.get("SERVICE")
  api = vc.get("API")
  family = vc.get("VOICE_FAMILY")

  if not text:
    raise ValueError(f"Config {idx}: TEXT is mandatory")

  if "[" in text and "]" in text and input_source == "TEXT":
    print(f"{WARN_COLOR}Warning Config {idx}: text in square brackets detected. MARKUP should be used for INPUT_SOURCE if Speech Markup tags are used.{RESET_COLOR}")

  # Identify configured speakers
  speakers = []
  for i in range(1, 4):
    s_name = vc.get(f"SPEAKER{i}_VOICE_NAME")
    s_alias = vc.get(f"SPEAKER{i}_ALIAS")
    if s_name:
      speakers.append({"name": s_name, "alias": s_alias})

  # Set compatibility field
  if speakers:
    vc["VOICE_NAME"] = speakers[0]["name"]

  # Check Voice Identifiers Presence
  voice_name = vc.get("VOICE_NAME")
  ssml_gender = vc.get("SSML_GENDER")
  voice_clone_key = vc.get("VOICE_CLONE_KEY")

  identifiers = []
  if voice_name: identifiers.append("VOICE_NAME")
  if ssml_gender: identifiers.append("SSML_GENDER")
  if voice_clone_key: identifiers.append("VOICE_CLONE_KEY")

  if not identifiers:
      raise ValueError(f"Config {idx}: At least one of VOICE_NAME, SSML_GENDER, or VOICE_CLONE_KEY must be configured.")
  if len(identifiers) > 1:
      print(f"{WARN_COLOR}Warning Config {idx}: Multiple voice identifiers configured ({', '.join(identifiers)}). This might cause ambiguity.{RESET_COLOR}")

  if service == "TTS":
    if vc.get("TEMPERATURE", 1.0) != 1.0:
      print(f"{WARN_COLOR}Warning Config {idx}: Temperature is only supported for Vertex AI and Gemini API.{RESET_COLOR}")

    if api not in ["synthesize", "synthesizeLongAudio", "streaming"]:
      raise ValueError(f"Config {idx}: API '{api}' is not supported for TTS service. Supported APIs: synthesize, synthesizeLongAudio, streaming")
    if family and family.lower().startswith("gemini") and api == "synthesizeLongAudio":
      raise ValueError(f"Config {idx}: Gemini voices do not support synthesizeLongAudio API")
    if vc.get("VOICE_SAMPLE_AUDIO"):
      raise ValueError(f"Config {idx}: VOICE_SAMPLE_AUDIO is only supported for Vertex API.")

  elif service in ["Vertex AI", "Gemini API"]:
    if api not in ["generateContent", "streamingGenerateContent"]:
      raise ValueError(f"Config {idx}: API '{api}' is not supported for {service} service. Supported APIs: generateContent, streamingGenerateContent")

    if vc.get("SPEAKING_RATE", 1.0) != 1.0:
      raise ValueError(f"Config {idx}: SPEAKING_RATE is only supported for TTS API.")
    if vc.get("PITCH", 0.0) != 0.0:
      raise ValueError(f"Config {idx}: PITCH is only supported for TTS API.")
    if vc.get("VOLUME_GAIN_DB", 0.0) != 0.0:
      raise ValueError(f"Config {idx}: VOLUME_GAIN_DB is only supported for TTS API.")
    if vc.get("EFFECTS_PROFILE_ID", "") != "":
      raise ValueError(f"Config {idx}: EFFECTS_PROFILE_ID is only supported for TTS API.")

  if service != "TTS":
    output_encoding = vc.get("OUTPUT_ENCODING", "LINEAR16")
    output_sample_rate = vc.get("OUTPUT_SAMPLE_RATE", "24000")
    if output_encoding not in ["PCM", "LINEAR16"] or str(output_sample_rate) != "24000":
      raise ValueError(f"Config {idx}: Vertex AI and Gemini API only support PCM with a sample rate of 24000")
    if family and not family.startswith("gemini"):
      raise ValueError(f"Config {idx}: For non-TTS services, VOICE_FAMILY must start with gemini")

  # Voice Name Validation
  if voice_name:
    if family == "Chirp3-HD" or (family and family.lower().startswith("gemini")):
      if voice_name not in CHIRP_GEMINI_NAMES:
         raise ValueError(f"Config {idx}: Voice '{voice_name}' is not supported for family '{family}'. Supported names are: {CHIRP_GEMINI_NAMES}")
    elif family in ["Standard", "Wavenet", "Neural2", "Studio", "News"]:
      if voice_name not in STANDARD_NAMES:
         raise ValueError(f"Config {idx}: Voice '{voice_name}' is not supported for family '{family}'. Supported names are: A-Z")
    elif family == "Polyglot":
      if voice_name not in POLYGLOT_NAMES:
         raise ValueError(f"Config {idx}: Voice '{voice_name}' is not supported for family '{family}'. Supported names are: 1")

  if input_source == "MULTI_SPEAKER_MARKUP":
     valid_speaker_found = False
     for s in speakers:
        if s["name"] and s["alias"]:
            valid_speaker_found = True
     if not valid_speaker_found:
         raise ValueError(f"Config {idx}: For MULTI_SPEAKER_MARKUP, at least one speaker must have both VOICE_NAME and ALIAS configured.")

     # Check Aliases
     valid_aliases = {s["alias"] for s in speakers if s["alias"]}
     for line in text.split('\n'):
        if line.strip():
            if ":" not in line:
                 raise ValueError(f"Config {idx}: Line '{line}' in TEXT does not follow the format 'Speaker-Alias: Text' required for MULTI_SPEAKER_MARKUP.")
            else:
                 current_alias = line.split(':', 1)[0].strip()
                 if current_alias not in valid_aliases:
                     raise ValueError(f"Config {idx}: Alias '{current_alias}' in TEXT is not defined in any Voice Configuration. Defined aliases: {list(valid_aliases)}")

     prompt = vc.get("PROMPT", "")
     prompt_len = len(prompt.encode("utf-8")) if prompt else 0
     if service == "TTS" and prompt_len > 4000:
       print(f"{WARN_COLOR}Warning Config {idx}: PROMPT length ({prompt_len} bytes) may exceed TTS limits (8000 bytes for text + promt)!{RESET_COLOR}")

# Voice Validation against API
logger.debug("Validating configured voices against API...")

if 'VOICE_CONFIGS' in globals() and any(vc.get("SERVICE") == "TTS" for vc in VOICE_CONFIGS):
  try:
      # Fetch all available voices once
      all_voices_response = tts_client.list_voices()
      # Create a map for quick lookup: name -> voice_object
      available_voices_map = {voice.name: voice for voice in all_voices_response.voices}

      for idx, vc in enumerate(VOICE_CONFIGS):
        if vc.get("SERVICE") == "TTS" and vc.get("VOICE_NAME"):
          # Construct the voice name as it is done in the synthesis step
          if "gemini" in vc.get("VOICE_FAMILY", ""):
            voice_name = vc['VOICE_NAME']
          else:
            voice_name = f"{vc['LOCALE']}-{vc['VOICE_FAMILY']}-{vc['VOICE_NAME']}"

          if voice_name in available_voices_map:
            voice_obj = available_voices_map[voice_name]
            natural_hz = voice_obj.natural_sample_rate_hertz
            current_hz = int(vc.get("OUTPUT_SAMPLE_RATE", "24000"))

            if natural_hz != current_hz:
              print(f"{WARN_COLOR}Info Config {idx}: Voice '{voice_name}' has a natural sample rate of {natural_hz}Hz, "
                          f"but synthesis is configured for {current_hz}Hz. "
                          f"The voice quality will be best for the naturalSampleRateHertz sample rate.{RESET_COLOR}")
          else:
            raise ValueError(f"Config {idx}: Voice '{voice_name}' was not found in the available voices list from the TTS API. "
                            "Please verify the LOCALE, VOICE_FAMILY, and VOICE_NAME.")
  except Exception as e:
      print(f"{WARN_COLOR}Warning: Could not validate voices against TTS API: {e}{RESET_COLOR}")

# @title Speech generation helper functions

import math
import concurrent.futures
from google.api_core import retry

# -------------------------------------------------------------------------
# AUDIO
# -------------------------------------------------------------------------

def add_wav_header(pcm_data, sample_rate=24000, num_channels=1, sample_width=2):
    buffer = io.BytesIO()
    with wave.open(buffer, 'wb') as wf:
        wf.setnchannels(num_channels)
        wf.setsampwidth(sample_width)
        wf.setframerate(sample_rate)
        wf.writeframes(pcm_data)
    return buffer.getvalue()

def concat_audio(audio_parts):
    """Concatenates a list of audio bytes (WAV or other). handles WAV header stripping."""
    if not audio_parts:
        return b""
    if len(audio_parts) == 1:
        return audio_parts[0]

    # Try to process as WAV first
    try:
        params = None
        frames = b""
        for part in audio_parts:
            with wave.open(io.BytesIO(part), 'rb') as w:
                if params is None:
                    params = w.getparams()
                frames += w.readframes(w.getnframes())

        if params:
            buffer = io.BytesIO()
            with wave.open(buffer, 'wb') as w:
                w.setparams(params)
                w.writeframes(frames)
            return buffer.getvalue()
    except wave.Error:
        # Fallback for non-WAV (e.g. MP3), just concatenate bytes
        pass

    return b"".join(audio_parts)

# -------------------------------------------------------------------------
# SYNTHESIS FUNCTION
# -------------------------------------------------------------------------

def run_speech_synthesis(voice_configs, project_id, location, output_bucket, gemini_api_key):
    generation_results = []
    default_retry = retry.Retry()

    logger.debug("Starting audio synthesis process")

    pbar = tqdm(voice_configs, desc="Generating Audio")
    for vc_idx, voice_config in enumerate(pbar):
        pbar.set_description(f"Generating Audio with service ({voice_config.get('SERVICE')})")
        logger.debug(f"Processing voice config: {voice_config}")

        # Extract Config
        synthesis_text = voice_config.get("TEXT")
        if not synthesis_text:
             logger.warning("No TEXT found in config, skipping.")
             continue

        synthesis_prompt = voice_config.get("PROMPT", "")
        input_source = voice_config.get("INPUT_SOURCE", "TEXT")
        output_encoding = voice_config.get("OUTPUT_ENCODING", "LINEAR16")
        sample_rate = int(voice_config.get("OUTPUT_SAMPLE_RATE", 24000))
        number_of_inferences = int(voice_config.get("COUNT", 1))

        # TTS Specific
        speaking_rate = float(voice_config.get("SPEAKING_RATE", 1.0))
        pitch = float(voice_config.get("PITCH", 0.0))
        volume_gain_db = float(voice_config.get("VOLUME_GAIN_DB", 0.0))
        effects_profile_id = voice_config.get("EFFECTS_PROFILE_ID", "")

        # Normalize text for Multi-Speaker Markup to handle escaped newlines
        if input_source == "MULTI_SPEAKER_MARKUP":
            synthesis_text = synthesis_text.replace('\\n', '\n')

        with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:
            futures = []

            match voice_config["SERVICE"]:
                case "TTS":
                    # Construct voice name standard (e.g. en-US-Neural2-A)
                    if "gemini" in voice_config.get("VOICE_FAMILY", ""):
                        voice_name = voice_config['VOICE_NAME']
                        model_name = voice_config["VOICE_FAMILY"]
                    else:
                        voice_name = f"{voice_config['LOCALE']}-{voice_config['VOICE_FAMILY']}-{voice_config['VOICE_NAME']}"
                        model_name = None

                    logger.debug(f"Configured TTS params: voice_name={voice_name}, model_name={model_name}")

                    voice_params = texttospeech.VoiceSelectionParams(
                        language_code=voice_config["LOCALE"],
                        name=voice_name,
                        model_name=model_name
                    )

                    if input_source == "MULTI_SPEAKER_MARKUP":
                        speakers = []
                        for i in range(1, 4):
                            s_name = voice_config.get(f"SPEAKER{i}_VOICE_NAME")
                            s_alias = voice_config.get(f"SPEAKER{i}_ALIAS")
                            if s_name and s_alias:
                                speakers.append(texttospeech.MultispeakerPrebuiltVoice(
                                    speaker_id=s_name,
                                    speaker_alias=s_alias
                                ))

                        if speakers:
                            voice_params.multi_speaker_voice_config = texttospeech.MultiSpeakerVoiceConfig(speaker_voice_configs=speakers)
                            voice_params.name = None

                    audio_config = texttospeech.AudioConfig(
                        audio_encoding=getattr(texttospeech.AudioEncoding, output_encoding),
                        sample_rate_hertz=sample_rate,
                        speaking_rate=speaking_rate,
                        pitch=pitch,
                        volume_gain_db=volume_gain_db,
                        effects_profile_id=[effects_profile_id] if effects_profile_id else []
                    )

                    synthesis_inputs = []

                    if input_source == "MULTI_SPEAKER_MARKUP":
                        text_len = len(synthesis_text.encode('utf-8'))
                        if text_len > 4000:
                             logger.info(f"Splitting text of length {text_len} bytes for Standard TTS API")
                             num_parts = math.ceil(text_len / 4000)
                             target_size = text_len / num_parts
                             current_turns = []
                             current_size = 0
                             lines = [l for l in synthesis_text.split('\n') if l.strip() and ":" in l]
                             for line in lines:
                                 parts = line.split(':', 1)
                                 turn = texttospeech.MultiSpeakerMarkup.Turn(
                                     speaker=parts[0].strip(),
                                     text=parts[1].strip()
                                 )
                                 line_bytes = len(line.encode('utf-8')) + 1
                                 if current_size + line_bytes > target_size:
                                     diff_include = abs((current_size + line_bytes) - target_size)
                                     diff_exclude = abs(current_size - target_size)
                                     if diff_exclude < diff_include and current_turns:
                                         synthesis_inputs.append(texttospeech.SynthesisInput(
                                             multi_speaker_markup=texttospeech.MultiSpeakerMarkup(turns=current_turns)
                                         ))
                                         current_turns = []
                                         current_size = 0
                                 current_turns.append(turn)
                                 current_size += line_bytes
                             if current_turns:
                                 synthesis_inputs.append(texttospeech.SynthesisInput(
                                     multi_speaker_markup=texttospeech.MultiSpeakerMarkup(turns=current_turns)
                                 ))
                        else:
                            turns = []
                            for line in synthesis_text.split('\n'):
                                if line.strip() and ":" in line:
                                    parts = line.split(':', 1)
                                    turns.append(texttospeech.MultiSpeakerMarkup.Turn(
                                        speaker=parts[0].strip(),
                                        text=parts[1].strip()
                                    ))
                            synthesis_inputs.append(texttospeech.SynthesisInput(
                                multi_speaker_markup=texttospeech.MultiSpeakerMarkup(turns=turns)
                            ))
                    else:
                        match input_source:
                            case "SSML":
                                synthesis_inputs.append(texttospeech.SynthesisInput(ssml=synthesis_text))
                            case "MARKUP":
                                synthesis_inputs.append(texttospeech.SynthesisInput(markup=synthesis_text))
                            case _:
                                synthesis_inputs.append(texttospeech.SynthesisInput(text=synthesis_text))

                    if "gemini" in voice_config.get("VOICE_FAMILY", ""):
                        for inp in synthesis_inputs:
                            inp.prompt = synthesis_prompt

                    def log_input_length(inp):
                        if inp.text: l = len(inp.text)
                        elif inp.ssml: l = len(inp.ssml)
                        elif inp.markup: l = len(inp.markup)
                        elif inp.multi_speaker_markup: l = sum(len(t.text) for t in inp.multi_speaker_markup.turns)
                        else: l = 0
                        logger.debug(f"Synthesis input length: {l}")

                    match voice_config.get("API"):
                        case "synthesizeLongAudio":
                            logger.debug("API: synthesizeLongAudio")
                            if not output_bucket:
                                logger.error("OUTPUT_BUCKET is required for synthesizeLongAudio")
                                continue

                            storage_client = storage.Client(project=project_id)
                            bucket = storage_client.bucket(output_bucket)
                            parent = f"projects/{project_id}/locations/{location}"

                            def long_audio_task(idx):
                                try:
                                    audio_parts = []
                                    for inp_idx, inp in enumerate(synthesis_inputs):
                                        log_input_length(inp)
                                        output_filename = f"long_audio_{voice_config['LOCALE']}_{voice_config['VOICE_NAME']}_{uuid.uuid4()}_part{inp_idx}.wav"
                                        output_uri = f"gs://{output_bucket}/{output_filename}"
                                        logger.debug(f"Output URI: {output_uri}")

                                        synthesize_long_audio_request = texttospeech.SynthesizeLongAudioRequest(
                                            parent=parent,
                                            input=inp,
                                            voice=voice_params,
                                            audio_config=audio_config,
                                            output_gcs_uri=output_uri
                                        )

                                        operation = tts_long_audio_client.synthesize_long_audio(
                                            request=synthesize_long_audio_request,
                                            retry=default_retry
                                        )
                                        operation.result(timeout=1800)

                                        blob = bucket.blob(output_filename)
                                        audio_content = blob.download_as_bytes()
                                        audio_parts.append(audio_content)

                                    full_audio = concat_audio(audio_parts)
                                    logger.debug(f"Long Audio inference {idx} successful")
                                    return {
                                        "service": voice_config["SERVICE"],
                                        "voice_config": voice_config,
                                        "audio": full_audio,
                                        "text": synthesis_text,
                                        "inference_index": vc_idx,
                                        "sample_index": idx
                                    }
                                except Exception as e:
                                    logger.error(f"Error in Long Audio Inference {idx+1}: {e}")
                                    return None

                            for i in range(number_of_inferences):
                                futures.append(executor.submit(long_audio_task, i))

                        case "streaming":
                            logger.debug("API: streaming")
                            streaming_config = texttospeech.StreamingSynthesizeConfig(
                                voice=voice_params,
                                streaming_audio_config=texttospeech.StreamingAudioConfig(
                                    audio_encoding=audio_config.audio_encoding,
                                    sample_rate_hertz=audio_config.sample_rate_hertz
                                )
                            )

                            def streaming_task(idx):
                                try:
                                    audio_parts = []
                                    for inp in synthesis_inputs:
                                        log_input_length(inp)
                                        def request_generator():
                                            yield texttospeech.StreamingSynthesizeRequest(
                                                streaming_config=streaming_config
                                            )
                                            yield texttospeech.StreamingSynthesizeRequest(
                                                input=inp
                                            )

                                        responses = tts_client.streaming_synthesize(requests=request_generator(), retry=default_retry)
                                        part_audio = b"".join([response.audio_content for response in responses])
                                        audio_parts.append(part_audio)

                                    full_audio = concat_audio(audio_parts)
                                    logger.debug(f"Streaming inference {idx} successful")
                                    return {
                                        "service": voice_config["SERVICE"],
                                        "voice_config": voice_config,
                                        "audio": full_audio,
                                        "text": synthesis_text,
                                        "inference_index": vc_idx,
                                        "sample_index": idx
                                    }
                                except Exception as e:
                                    logger.error(f"Error in Streaming Inference {idx+1}: {e}")
                                    return None

                            for i in range(number_of_inferences):
                                futures.append(executor.submit(streaming_task, i))

                        case _:
                            logger.debug("API: synthesize_speech (standard)")
                            def standard_task(idx):
                                try:
                                    audio_parts = []
                                    for inp in synthesis_inputs:
                                        log_input_length(inp)
                                        response = tts_client.synthesize_speech(
                                            input=inp,
                                            voice=voice_params,
                                            audio_config=audio_config,
                                            retry=default_retry
                                        )
                                        audio_parts.append(response.audio_content)

                                    full_audio = concat_audio(audio_parts)
                                    logger.debug(f"Standard inference {idx} successful")
                                    return {
                                        "service": voice_config["SERVICE"],
                                        "voice_config": voice_config,
                                        "audio": full_audio,
                                        "text": synthesis_text,
                                        "inference_index": vc_idx,
                                        "sample_index": idx
                                    }
                                except Exception as e:
                                    logger.error(f"Error in TTS Inference {idx+1}: {e}")
                                    return None

                            for i in range(number_of_inferences):
                                futures.append(executor.submit(standard_task, i))

                case "Vertex AI" | "Gemini API":
                    logger.debug(f"Service: {voice_config['SERVICE']}")

                    http_options = types.HttpOptions(
                        headers=sherlog_header['headers'],
                    )

                    if voice_config["SERVICE"] == "Vertex AI":
                        genai_client = genai.Client(vertexai=True, project=project_id, location=location, http_options=http_options)
                    else:
                        genai_client = genai.Client(vertexai=False, api_key=gemini_api_key, http_options=http_options)

                    if synthesis_prompt:
                        contents = f"{synthesis_prompt}:\n{synthesis_text}"
                    else:
                        contents = synthesis_text

                    # Prepare Speech Config
                    speech_config = None

                    if input_source == "MULTI_SPEAKER_MARKUP":
                        speaker_voice_configs = []
                        for spk_i in range(1, 4):
                            s_name = voice_config.get(f"SPEAKER{spk_i}_VOICE_NAME")
                            s_alias = voice_config.get(f"SPEAKER{spk_i}_ALIAS")
                            if s_name and s_alias:
                                speaker_voice_configs.append(
                                    types.SpeakerVoiceConfig(
                                        speaker=s_alias,
                                        voice_config=types.VoiceConfig(
                                            prebuilt_voice_config=types.PrebuiltVoiceConfig(
                                                voice_name=s_name
                                            )
                                        )
                                    )
                                )

                        if speaker_voice_configs:
                            speech_config = types.SpeechConfig(
                                multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
                                    speaker_voice_configs=speaker_voice_configs
                                )
                            )

                    # Fallback to Single Speaker if not multi-speaker or no speakers configured
                    if not speech_config:
                        speech_config = types.SpeechConfig(
                            voice_config=types.VoiceConfig(
                                prebuilt_voice_config=types.PrebuiltVoiceConfig(
                                    voice_name=voice_config.get("VOICE_NAME", "")
                                )
                            )
                        )

                    config = types.GenerateContentConfig(
                        temperature=voice_config.get("TEMPERATURE", 1) if voice_config.get("TEMPERATURE", 1) != 1 else None,
                        speech_config=speech_config,
                        response_modalities = ["AUDIO"]
                    )

                    def gemini_task(idx):
                        try:
                            raw_audio_chunks = []

                            def call_generate():
                                if voice_config.get("API") == "streamingGenerateContent":
                                    logger.debug(f"Calling generate_content_stream with model {voice_config.get('VOICE_FAMILY')}")
                                    return list(genai_client.models.generate_content_stream(
                                        model=voice_config.get("VOICE_FAMILY"),
                                        contents=contents,
                                        config=config
                                    ))
                                else: # Default to generateContent
                                    logger.debug(f"Calling generate_content with model {voice_config.get('VOICE_FAMILY')}")
                                    return genai_client.models.generate_content(
                                        model=voice_config.get("VOICE_FAMILY"),
                                        contents=contents,
                                        config=config
                                    )

                            response_obj = call_generate()

                            if voice_config.get("API") == "streamingGenerateContent":
                                responses = response_obj # already a list from call_generate
                                for response in responses:
                                    if hasattr(response, 'sdk_http_response') and 'x-goog-sherlog-link' in response.sdk_http_response.headers:
                                        link = response.sdk_http_response.headers['x-goog-sherlog-link']
                                        logger.info(f"Sherlog link: {link}")

                                    if response.candidates:
                                        for candidate in response.candidates:
                                            if candidate.content and candidate.content.parts:
                                                for part in candidate.content.parts:
                                                    if part.inline_data:
                                                        raw_audio_chunks.append(part.inline_data.data)

                            else:
                                response = response_obj
                                if hasattr(response, 'sdk_http_response') and 'x-goog-sherlog-link' in response.sdk_http_response.headers:
                                    link = response.sdk_http_response.headers['x-goog-sherlog-link']
                                    logger.info(f"Sherlog link: {link}")
                                else:
                                    logger.debug('No Sherlog link found')

                                if response.candidates and response.candidates[0].content.parts:
                                    for part in response.candidates[0].content.parts:
                                        if part.inline_data:
                                            raw_audio_chunks.append(part.inline_data.data)

                            if raw_audio_chunks:
                                raw_audio = b"".join(raw_audio_chunks)
                                wav_audio = add_wav_header(raw_audio, int(sample_rate))
                                logger.debug(f"Gemini inference {idx} successful")
                                return {
                                    "service": voice_config["SERVICE"],
                                    "voice_config": voice_config,
                                    "audio": wav_audio,
                                    "text": synthesis_text,
                                    "inference_index": vc_idx,
                                    "sample_index": idx
                                }
                            else:
                                logger.warning(f"No audio content generated for inference {idx}")
                                return None

                        except Exception as e:
                            logger.error(f"Error in Gemini Inference {idx+1}: {e}")
                            return None

                    for i in range(number_of_inferences):
                        futures.append(executor.submit(gemini_task, i))

            # Wait for results
            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="Running inference", leave=False):
                try:
                    res = future.result()
                    if res:
                        generation_results.append(res)
                except Exception as e:
                    logger.error(f"An unexpected error occurred in worker: {e}")

    return generation_results

# @title Evaluation helper functions

import math
import concurrent.futures
from google.api_core import retry
import google.auth.transport.requests

# -------------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------------

TAG_PATTERN = re.compile(r'\[.*?\]', re.IGNORECASE)

# -------------------------------------------------------------------------
# AUDIO ANALYSIS
# -------------------------------------------------------------------------

class AudioMetrics(BaseModel):
    mean_pitch_hz: float = 0.0
    pitch_std_hz: float = 0.0
    pitch_range_hz: float = 0.0
    jitter_percent: float = 0.0
    shimmer_db: float = 0.0
    hnr_db: float = 0.0
    estimated_tempo_bpm: float = 0.0
    duration_sec: float = 0.0

def run_audio_analysis(audio_bytes, sample_rate):
    """
    Runs audio analysis on bytes directly.
    """
    metrics = AudioMetrics()

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio:
        temp_audio.write(audio_bytes)
        temp_audio_path = temp_audio.name

    try:
        # --- Parselmouth Analysis (Pitch, Voice Quality) ---
        snd = parselmouth.Sound(temp_audio_path)
        pitch = snd.to_pitch()
        pitch_values = pitch.selected_array['frequency']
        voiced_pitch = pitch_values[pitch_values > 0]

        if len(voiced_pitch) > 0:
            metrics.mean_pitch_hz = float(np.mean(voiced_pitch))
            metrics.pitch_std_hz = float(np.std(voiced_pitch))
            metrics.pitch_range_hz = float(np.max(voiced_pitch) - np.min(voiced_pitch))

        # Voice quality metrics
        try:
            point_process = call(snd, "To PointProcess (periodic, cc)", 75, 500)
            jitter = call(point_process, "Get jitter (local)", 0, 0, 0.0001, 0.02, 1.3) * 100
            shimmer = call([snd, point_process], "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6)
            hnr = call(snd, "To Harmonicity (cc)", 0.01, 75, 0.1, 1.0).values.mean()

            metrics.jitter_percent = float(jitter) if not np.isnan(jitter) else 0.0
            metrics.shimmer_db = float(shimmer) if not np.isnan(shimmer) else 0.0
            metrics.hnr_db = float(hnr) if not np.isnan(hnr) else 0.0
        except Exception as e:
            logger.warning(f"Voice quality analysis failed: {e}")

        # --- Librosa Analysis (Rhythm, Duration) ---
        try:
            y, sr = librosa.load(temp_audio_path)
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            if isinstance(tempo, np.ndarray):
                tempo = tempo[0]
            metrics.estimated_tempo_bpm = float(tempo)
            metrics.duration_sec = float(librosa.get_duration(y=y, sr=sr))
        except Exception as e:
             logger.warning(f"Librosa analysis failed: {e}")

    finally:
        # Clean up temp file
        if os.path.exists(temp_audio_path):
            os.remove(temp_audio_path)

    # Return as dict for JSON serialization
    try:
        return metrics.model_dump()
    except AttributeError:
        return metrics.dict()

# -------------------------------------------------------------------------
# TEXT & TOKENIZATION UTILS
# -------------------------------------------------------------------------

def clean_text_for_comparison(text):
    text = TAG_PATTERN.sub(' ', text)
    text = re.sub(r'<[^>]+>', '', text) # SSML
    # Modified to remove dashes (removed '-' from the character class)
    # This normalizes "Produkt-Experte" to "produktexperte" and "Mercedes-Benz" to "mercedesbenz"
    text = re.sub(r'[^\w\s]', '', text).lower()
    return " ".join(text.split())

def is_cjk(char):
    if not char: return False
    return ('\u4e00' <= char <= '\u9fff' or
            '\u3040' <= char <= '\u309f' or
            '\u30a0' <= char <= '\u30ff' or
            '\uac00' <= char <= '\ud7af')

def flatten_stt_result(stt_result):
    tokens = []
    if not stt_result.get("results"):
        return tokens

    for result in stt_result.get("results", []):
        alts = result.get("alternatives", [])
        if not alts: continue
        alt = alts[0]

        for w in alt.get("words", []):
            start_raw = w.get("startOffset") or w.get("start_offset", "0s")
            end_raw = w.get("endOffset") or w.get("end_offset", "0s")

            def parse_time(t_str):
                if isinstance(t_str, (int, float)): return float(t_str)
                if isinstance(t_str, str): return float(t_str.rstrip('s'))
                return 0.0

            start_t = parse_time(start_raw)
            end_t = parse_time(end_raw)
            text = w.get("word", "")

            if any(is_cjk(c) for c in text) and len(text) > 1:
                duration = end_t - start_t
                char_duration = duration / len(text)
                for idx, char in enumerate(text):
                    tokens.append({
                        "text": char,
                        "raw_text": text,
                        "start": start_t + (idx * char_duration),
                        "end": start_t + ((idx + 1) * char_duration),
                        "is_cjk": True
                    })
            else:
                tokens.append({
                    "text": text,
                    "raw_text": text,
                    "start": start_t,
                    "end": end_t,
                    "is_cjk": False
                })
    return tokens

def tokenize_reference(text):
    raw_words = text.split()
    ref_tokens = []
    for word in raw_words:
        clean = clean_text_for_comparison(word)
        if any(is_cjk(c) for c in clean):
            for char in clean:
                ref_tokens.append({"text": char, "display": char, "is_cjk": True})
        else:
            if clean:
                ref_tokens.append({"text": clean, "display": word, "is_cjk": False})
    return ref_tokens

# -------------------------------------------------------------------------
# GEMINI VALIDATION & ANALYSIS
# -------------------------------------------------------------------------

def batch_check_normalizations(mismatches, client, model_name):
    if not mismatches:
        return {}

    prompt = """
    You are a linguistic expert validating Speech-to-Text accuracy.
    Compare the Reference (ref) and Hypothesis (hyp).

    Determine the relationship:
    1. FULL_MATCH: 'hyp' is a valid normalization, spelling variant, number conversion,
       contraction, or acceptable expansion of 'ref'.
       (e.g. "TUI466" <-> "TUI 466", "we are" <-> "we're", "8 8" <-> "88")
    2. PARTIAL_MATCH: 'hyp' *contains* a valid normalization of 'ref' but has extra words
       (insertions) at the start or end.
       (e.g. Ref: "TUI 466", Hyp: "T UI 466 T" -> Match: "T UI 466")
    3. NO_MATCH: The meanings differ significantly or it is an error.
       (e.g. "cat" <-> "dog", "TUI466" <-> "TUI 400 66")

    Respond ONLY with a JSON map where keys are the IDs provided in the input and values are objects:
    {
       "0": {"type": "FULL_MATCH"},
       "1": {"type": "PARTIAL_MATCH", "matched_hyp": "extracted text substring from hyp"},
       "2": {"type": "NO_MATCH"}
    }
    """
    mini_payload = [{k: v for k, v in m.items()} for m in mismatches]

    try:
        response = client.models.generate_content(
            model=model_name,
            contents=[prompt, json.dumps(mini_payload, ensure_ascii=False)],
            config=types.GenerateContentConfig(response_mime_type="application/json")
        )
        result = json.loads(response.text)
        if isinstance(result, list):
            logger.warning(f"Batch normalization returned a list instead of a dict. Attempting to merge.")
            merged = {}
            for item in result:
                if isinstance(item, dict):
                    merged.update(item)
            return merged
        return result
    except Exception as e:
        logger.error(f"Gemini API Error: {e}")
        return {str(m['id']): {"type": "NO_MATCH"} for m in mismatches}

def find_subsequence(tokens, target_text):
    target_clean = clean_text_for_comparison(target_text)
    if not target_clean:
        return None, None
    n = len(tokens)
    for length in range(n, 0, -1):
        for start in range(n - length + 1):
            sub = tokens[start : start+length]
            sub_clean_parts = [clean_text_for_comparison(t['text']) for t in sub]
            sub_text = " ".join([p for p in sub_clean_parts if p])
            if sub_text == target_clean:
                return start, start + length
    return None, None

def process_stt_results(stt_results_list, reference_text, client, model_name):
    if not stt_results_list:
        return []

    clean_ref_text = re.sub(TAG_PATTERN, ' ', reference_text).strip()
    ref_objs = tokenize_reference(clean_ref_text)
    ref_texts = [r['text'] for r in ref_objs]

    stt_data_list = [flatten_stt_result(res) for res in stt_results_list]

    mismatches = []
    mismatch_registry = {}
    all_opcodes = []

    for idx, stt_tokens in enumerate(stt_data_list):
        stt_texts = [clean_text_for_comparison(t['text']) for t in stt_tokens]
        matcher = difflib.SequenceMatcher(None, ref_texts, stt_texts)
        opcodes = matcher.get_opcodes()
        all_opcodes.append(opcodes)

        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'replace':
                ref_phrase = " ".join(ref_texts[i1:i2])
                hyp_phrase = " ".join(stt_texts[j1:j2])
                sig = f"{ref_phrase}||{hyp_phrase}"
                if sig not in mismatch_registry:
                    m_id = len(mismatches)
                    mismatches.append({"id": m_id, "ref": ref_phrase, "hyp": hyp_phrase})
                    mismatch_registry[sig] = m_id

    valid_norms = batch_check_normalizations(mismatches, client, model_name)

    def get_norm_result(r_txt, h_txt):
        sig = f"{r_txt}||{h_txt}"
        if sig in mismatch_registry:
            mid = mismatch_registry[sig]
            res = valid_norms.get(str(mid)) or valid_norms.get(mid)
            if res:
                return res
        return {"type": "NO_MATCH"}

    ref_status = [None] * len(ref_objs)

    for stt_idx, opcodes in enumerate(all_opcodes):
        stt_tokens = stt_data_list[stt_idx]
        stt_texts = [clean_text_for_comparison(t['text']) for t in stt_tokens]

        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'equal':
                for k in range(i1, i2):
                    h_idx = j1 + (k - i1)
                    if ref_status[k] is None:
                        ref_status[k] = {
                            'found': True,
                            'start': stt_tokens[h_idx]['start'],
                            'end': stt_tokens[h_idx]['end'],
                            'hyp_text': stt_tokens[h_idx]['text']
                        }
            elif tag == 'replace':
                ref_p = " ".join(ref_texts[i1:i2])
                hyp_p = " ".join(stt_texts[j1:j2])
                norm_res = get_norm_result(ref_p, hyp_p)
                match_type = norm_res.get("type", "NO_MATCH")

                if match_type == "FULL_MATCH":
                    seg_start = stt_tokens[j1]['start']
                    seg_end = stt_tokens[j2-1]['end']
                    duration = seg_end - seg_start
                    count = i2 - i1
                    step = duration / max(count, 1)
                    for r_offset in range(count):
                        k = i1 + r_offset
                        if ref_status[k] is None:
                            ref_status[k] = {
                                'found': True,
                                'start': seg_start + (r_offset * step),
                                'end': seg_start + ((r_offset + 1) * step),
                                'hyp_text': ref_objs[k]['display']
                            }

    final_sequence = []
    primary_stt_tokens = stt_data_list[0]
    primary_opcodes = all_opcodes[0]
    last_end_time = 0.0
    if primary_stt_tokens:
        last_end_time = primary_stt_tokens[0]['start']

    for tag, i1, i2, j1, j2 in primary_opcodes:
        if tag == 'equal':
            for k in range(j1, j2):
                token = primary_stt_tokens[k]
                final_sequence.append({"text": token['text'], "start": token['start'], "end": token['end'], "status": "correct"})
                last_end_time = token['end']
        elif tag == 'insert':
            for k in range(j1, j2):
                token = primary_stt_tokens[k]
                final_sequence.append({"text": token['text'], "start": token['start'], "end": token['end'], "status": "insertion"})
                last_end_time = token['end']
        elif tag == 'delete':
            for k in range(i1, i2):
                status_obj = ref_status[k]
                if status_obj and status_obj['found']:
                    final_sequence.append({"text": ref_objs[k]['display'], "start": status_obj['start'], "end": status_obj['end'], "status": "correct"})
                    last_end_time = status_obj['end']
                else:
                    final_sequence.append({"text": ref_objs[k]['display'], "start": last_end_time, "end": last_end_time, "status": "missing"})
        elif tag == 'replace':
            ref_p = " ".join(ref_texts[i1:i2])
            segment_tokens = primary_stt_tokens[j1:j2]
            hyp_p = " ".join([clean_text_for_comparison(t['text']) for t in segment_tokens])
            norm_res = get_norm_result(ref_p, hyp_p)
            match_type = norm_res.get("type", "NO_MATCH")
            processed_partial = False

            if match_type == "PARTIAL_MATCH":
                matched_hyp_text = norm_res.get("matched_hyp", "")
                sub_start, sub_end = find_subsequence(segment_tokens, matched_hyp_text)
                if sub_start is not None:
                    processed_partial = True
                    for k in range(j1, j1 + sub_start):
                        token = primary_stt_tokens[k]
                        final_sequence.append({"text": token['text'], "start": token['start'], "end": token['end'], "status": "insertion"})
                        last_end_time = token['end']
                    seg_start_t = primary_stt_tokens[j1 + sub_start]['start']
                    seg_end_t = primary_stt_tokens[j1 + sub_end - 1]['end']
                    duration = seg_end_t - seg_start_t
                    step = duration / max((i2-i1), 1)
                    for idx, r_idx in enumerate(range(i1, i2)):
                        final_sequence.append({"text": ref_objs[r_idx]['display'], "start": seg_start_t + (idx * step), "end": seg_start_t + ((idx + 1) * step), "status": "correct"})
                    last_end_time = seg_end_t
                    for k in range(j1 + sub_end, j2):
                        token = primary_stt_tokens[k]
                        final_sequence.append({"text": token['text'], "start": token['start'], "end": token['end'], "status": "insertion"})
                        last_end_time = token['end']

            if match_type == "FULL_MATCH" or (match_type == "PARTIAL_MATCH" and not processed_partial):
                if match_type == "FULL_MATCH":
                    seg_start = primary_stt_tokens[j1]['start']
                    seg_end = primary_stt_tokens[j2-1]['end']
                    duration = seg_end - seg_start
                    step = duration / max((i2-i1), 1)
                    for idx, r_idx in enumerate(range(i1, i2)):
                        final_sequence.append({"text": ref_objs[r_idx]['display'], "start": seg_start + (idx * step), "end": seg_start + ((idx + 1) * step), "status": "correct"})
                    last_end_time = seg_end
                else:
                    match_type = "NO_MATCH"

            if match_type == "NO_MATCH":
                for k in range(i1, i2):
                    status_obj = ref_status[k]
                    if status_obj and status_obj['found']:
                        final_sequence.append({"text": ref_objs[k]['display'], "start": status_obj['start'], "end": status_obj['end'], "status": "correct"})
                    else:
                        final_sequence.append({"text": ref_objs[k]['display'], "start": last_end_time, "end": last_end_time, "status": "missing"})
                for k in range(j1, j2):
                    token = primary_stt_tokens[k]
                    final_sequence.append({"text": token['text'], "start": token['start'], "end": token['end'], "status": "insertion"})
                    last_end_time = token['end']
    return final_sequence

def run_batch_stt_inference(stt_tasks, project_id, output_bucket, credentials):
    stt_outputs = {}
    storage_client = storage.Client(project=project_id)
    bucket = storage_client.bucket(output_bucket)

    from google.api_core import retry

    for model, items in stt_tasks.items():
        logger.debug(f"Processing STT batch for Model: {model}, Total Items: {len(items)}")
        if model == "chirp_3": stt_location = "us"
        elif model == "chirp_2": stt_location = "us-central1"
        else: stt_location = "global"

        api_endpoint = f"speech.googleapis.com" if stt_location == "global" else f"{stt_location}-speech.googleapis.com"
        client_options = ClientOptions(api_endpoint=api_endpoint, quota_project_id=project_id)
        speech_client = speech_v2.SpeechClient(client_options=client_options)

        parent = f"projects/{project_id}/locations/{stt_location}"
        recognizer_name = f"{parent}/recognizers/_"

        items_by_lang = {}
        for item in items:
            l = item['lang']
            if l not in items_by_lang: items_by_lang[l] = []
            items_by_lang[l].append(item)

        for lang, lang_items in items_by_lang.items():
            recognition_config = cloud_speech.RecognitionConfig(
                auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),
                language_codes=[lang], model=model,
                features=cloud_speech.RecognitionFeatures(enable_automatic_punctuation=True, enable_word_time_offsets=True)
            )

            pending_ops = []
            chunk_size = 2

            # Create chunks
            chunks = [lang_items[i:i + chunk_size] for i in range(0, len(lang_items), chunk_size)]

            logger.info(f"Submitting {len(chunks)} batch operations for {model} ({lang}) in parallel...")

            for chunk in chunks:
                batch_files = []
                temp_mappings = []

                for item in chunk:
                    idx = item['index']
                    gen_res = item['gen_result']
                    fname = f"eval_audio_{idx}_{uuid.uuid4()}.wav"
                    blob = bucket.blob(fname)
                    blob.upload_from_string(gen_res["audio"], content_type="audio/wav")
                    input_uri = f"gs://{output_bucket}/{fname}"
                    batch_files.append(cloud_speech.BatchRecognizeFileMetadata(uri=input_uri))
                    temp_mappings.append((idx, fname))

                batch_id = f"batch-{uuid.uuid4()}"
                output_prefix = f"gs://{output_bucket}/stt_results/{batch_id}/"

                try:
                    operation = speech_client.batch_recognize(
                        request=cloud_speech.BatchRecognizeRequest(
                            recognizer=recognizer_name, config=recognition_config, files=batch_files,
                            recognition_output_config=cloud_speech.RecognitionOutputConfig(gcs_output_config=cloud_speech.GcsOutputConfig(uri=output_prefix))
                        ),
                        retry=retry.Retry()
                    )
                    pending_ops.append((operation, temp_mappings))
                except Exception as e:
                    logger.error(f"Batch Recognize Submission Failed: {e}")

            # Wait for results
            for operation, temp_mappings in tqdm(pending_ops, desc=f"Waiting for Batch STT {model} {lang}", leave=False):
                try:
                    logger.debug(operation.operation)
                    batch_response = operation.result()

                    for idx, audio_fname in temp_mappings:
                        input_uri = f"gs://{output_bucket}/{audio_fname}"
                        if input_uri in batch_response.results:
                            result_file_info = batch_response.results[input_uri]
                            output_uri = result_file_info.uri
                            match = re.match(r"gs://([^/]+)/(.*)", output_uri)
                            if match:
                                out_bucket_name, out_blob_name = match.group(1, 2)
                                res_bucket = storage_client.bucket(out_bucket_name) if out_bucket_name != output_bucket else bucket
                                result_blob = res_bucket.blob(out_blob_name)
                                if result_blob.exists():
                                    stt_outputs[(idx, model)] = {
                                        "result_data": json.loads(result_blob.download_as_bytes()),
                                        "audio_uri": f"https://storage.googleapis.com/{output_bucket}/{audio_fname}"
                                    }
                except Exception as e:
                    logger.error(f"Batch Recognize Operation Failed: {e}")
    return stt_outputs

def generate_gemini_analysis(client, model_name, audio_bytes, reference_text, analysis_data, audio_metrics, issue_description="", input_source="TEXT"):
    markup_instruction = ""
    if input_source == "MARKUP" or input_source == "MULTI_SPEAKER_MARKUP":
        markup_instruction = (
            "IMPORTANT: Any text enclosed in square brackets (e.g. [pause], [laugh]) are control tags and must NOT be spoken. "
            "If these tags (or parts of them) appear in the 'text' fields of the Analysis Data, this is a critical failure."
        )

    issue_instruction = ""
    if issue_description:
        issue_instruction = f"Check if the following reported issue is present in the audio: '{issue_description}'. "

    prompt = f"""
    You are an expert Audio and Speech analyst.
    Analyze the provided Audio and the Speech-to-Text alignment data (Analysis Data).
    Reference Text: "{reference_text}"

    {issue_instruction}
    {markup_instruction}

    Audio Quality Metrics:
    {json.dumps(audio_metrics)}

    Analysis Data (JSON):
    {json.dumps(analysis_data)}

    Provide the output in the following JSON format:
    {{
        "correct": boolean,
        "reproduced": boolean,
        "analysis": string
    }}

    Definitions:
    - correct: True if the audio matches the text perfectly, has high quality, correct pronunciation and intonation, and NO reported issues are reproduced.
    - reproduced: True if the reported issue (if any) is clearly heard in the audio. If no issue reported, set to False.
    - analysis: Detailed analysis of the alignment, audio quality, pronunciation, intonation, and whether the issue was reproduced.
    """

    contents = [types.Part.from_bytes(data=audio_bytes, mime_type="audio/wav"), prompt]
    try:
        response = client.models.generate_content(
            model=model_name,
            contents=contents,
            config=types.GenerateContentConfig(response_mime_type="application/json", response_schema={"type": "OBJECT", "properties": {"correct": {"type": "BOOLEAN"}, "reproduced": {"type": "BOOLEAN"}, "analysis": {"type": "STRING"}}, "required": ["correct", "reproduced", "analysis"]})
        )
        return json.loads(response.text)
    except Exception as e:
        logger.warning(f"Gemini attempt failed: {e}")
        return {"analysis": f"Error: {e}", "correct": False, "reproduced": False}

# -------------------------------------------------------------------------
# VTT UTILS
# -------------------------------------------------------------------------

def seconds_to_vtt_time(seconds):
    td = datetime.timedelta(seconds=seconds)
    total_seconds = int(td.total_seconds())
    millis = int(td.microseconds / 1000)
    return f"{total_seconds // 3600:02}:{(total_seconds % 3600) // 60:02}:{total_seconds % 60:02}.{millis:03}"

def create_vtt(analyzed_sequence):
    vtt_lines = ["WEBVTT\n"]
    if not analyzed_sequence: return "WEBVTT\n\nNOTE No content"
    current_cue = []
    for i, token in enumerate(analyzed_sequence):
        current_cue.append(token)
        txt = str(token["text"])
        is_punctuation = re.search(r'[.,?!:;]$', txt) is not None
        is_ellipsis = txt.endswith("...")
        is_last = (i == len(analyzed_sequence) - 1)
        should_split = (is_punctuation and not is_ellipsis) or is_last or len(current_cue) > 15

        if should_split:
            start_t = current_cue[0]["start"]
            end_t = current_cue[-1]["end"]
            if end_t <= start_t: end_t = start_t + 1.0
            vtt_lines.append(f"{seconds_to_vtt_time(start_t)} --> {seconds_to_vtt_time(end_t)}")
            payload = []
            for t in current_cue:
                ts = seconds_to_vtt_time(t["start"])
                content = t["text"]
                if t["status"] == "missing": content = f'<span style="color:IndianRed">{content}</span>'
                elif t["status"] == "insertion": content = f'<span style="color:CornflowerBlue">{content}</span>'
                payload.append(f"<{ts}>{content}")
            vtt_lines.append(" ".join(payload))
            vtt_lines.append("")
            current_cue = []
    return "\n".join(vtt_lines)

def create_multi_speaker_vtt(analysis_data, turns_metadata):
    vtt_lines = ["WEBVTT\n"]
    if not analysis_data: return "WEBVTT\n\nNOTE No content"
    ref_token_speaker_map = []
    for turn in turns_metadata:
        spk = turn['speaker']
        for _ in range(turn['num_tokens']): ref_token_speaker_map.append(spk)

    current_ref_idx = 0
    current_cue_tokens = []
    current_speaker = ref_token_speaker_map[0] if ref_token_speaker_map else ""

    def flush_cue(tokens, speaker_label):
        if not tokens: return
        start_t = tokens[0]['start']
        end_t = tokens[-1]['end']
        if end_t <= start_t: end_t = start_t + 1.0
        vtt_lines.append(f"{seconds_to_vtt_time(start_t)} --> {seconds_to_vtt_time(end_t)}")
        payload = [f"<b>{speaker_label}:</b>"]
        for t in tokens:
            ts = seconds_to_vtt_time(t["start"])
            content = str(t["text"])
            if t["status"] == "missing": content = f'<span style="color:IndianRed">{content}</span>'
            elif t["status"] == "insertion": content = f'<span style="color:CornflowerBlue">{content}</span>'
            payload.append(f"<{ts}>{content}")
        vtt_lines.append(" ".join(payload))
        vtt_lines.append("")

    for i, token in enumerate(analysis_data):
        token_speaker = current_speaker
        if token['status'] in ['correct', 'missing']:
            if current_ref_idx < len(ref_token_speaker_map):
                token_speaker = ref_token_speaker_map[current_ref_idx]
        if token_speaker != current_speaker and current_cue_tokens:
             flush_cue(current_cue_tokens, current_speaker)
             current_cue_tokens = []
             current_speaker = token_speaker
        current_cue_tokens.append(token)
        if token['status'] in ['correct', 'missing']:
            current_ref_idx += 1
        if len(current_cue_tokens) > 30 or (i == len(analysis_data) - 1):
            flush_cue(current_cue_tokens, current_speaker)
            current_cue_tokens = []
    return "\n".join(vtt_lines)

def get_downscoped_token(bucket_name):
    try:
        # Get base credentials (scoped to cloud-platform)
        base_credentials, _ = google.auth.default(
            scopes=["https://www.googleapis.com/auth/cloud-platform"]
        )

        request = google.auth.transport.requests.Request()

        # Explicitly refresh source credentials to ensure they are valid
        try:
             base_credentials.refresh(request)
        except Exception as e:
             # Log debug but continue, as some creds don't need/support explicit refresh or auto-refresh
             logger.debug(f"Source credential refresh skipped/failed: {e}")

        # Restrict access ONLY to the specific bucket with Object Viewer permissions.
        # The resource name format for buckets in IAM conditions is usually projects/_/buckets/BUCKET_NAME
        # or //storage.googleapis.com/projects/_/buckets/BUCKET_NAME
        rule = downscoped.AccessBoundaryRule(
            available_resource=f"//storage.googleapis.com/projects/_/buckets/{bucket_name}",
            available_permissions=["inRole:roles/storage.objectViewer"]
        )
        access_boundary = downscoped.CredentialAccessBoundary(rules=[rule])

        downscoped_creds = downscoped.Credentials(
            source_credentials=base_credentials,
            credential_access_boundary=access_boundary
        )

        downscoped_creds.refresh(request)
        return downscoped_creds.token
    except Exception as e:
        logger.error(f"Failed to generate downscoped token: {e}")
        return ""

HTML_TEMPLATE="""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Embedded Transcript Player</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        .balanced-text { text-wrap: balance; }
        video::-webkit-media-text-track-display { display: none !important; }
        ::cue { background: transparent; color: transparent; }
        .transcript-container::-webkit-scrollbar { width: 4px; }
        .transcript-container::-webkit-scrollbar-thumb { background-color: #e5e7eb; border-radius: 4px; }
        .active-line { background-color: #f8fafc; border-left: 3px solid #3b82f6; transition: all 0.2s ease; }
        .word { transition: background-color 0.1s ease; display: inline-block; padding: 0 2px; border-radius: 3px; }
        .active-word { background-color: #dbeafe; font-weight: 600; box-shadow: 0 0 0 1px #bfdbfe; }
        .transcript-line b { font-weight: bold; } .transcript-line i { font-style: italic; } .transcript-line u { text-decoration: underline; }
    </style>
</head>
<body class="bg-white p-2 font-sans">
    <div class="w-full max-w-3xl mx-auto border border-gray-200 rounded-xl shadow-sm p-4 space-y-4">
        <header class="flex justify-between items-center border-b border-gray-100 pb-2">
            <h1 class="text-md font-bold text-gray-800">__TITLE__</h1>
            <span class="text-xs text-gray-400 uppercase tracking-wide">__CONFIG_NAME__</span>
        </header>
        <div class="player-container space-y-4">
            <audio class="audio-player w-full h-8" controls></audio>
            <div class="transcript-view transcript-container h-48 overflow-y-auto border border-gray-100 rounded-lg py-3 px-3 space-y-1 relative text-sm leading-relaxed">
            </div>
        </div>
    </div>
    <script>
        (function() {
            const TIME_OFFSET = 0.2;
            const container = document.currentScript.previousElementSibling;
            const player = container.querySelector('.audio-player');
            const transcriptView = container.querySelector('.transcript-view');
            const audioDataUri = "__AUDIO_URI__";
            const vttDataUri = "data:text/vtt;base64,__VTT_BASE64__";
            function parseVttTime(vttTime) {
                const parts = vttTime.trim().split(':');
                let seconds = 0;
                if (parts.length === 3) { seconds += parseInt(parts[0]) * 3600; seconds += parseInt(parts[1]) * 60; seconds += parseFloat(parts[2]); }
                else { seconds += parseInt(parts[0]) * 60; seconds += parseFloat(parts[1]); }
                return seconds;
            }
            function parseWordTimestamps(text) {
                const timestampRegex = /<(\\d{2}:\\d{2}(?::\\d{2})?\\.\\d{3})>/g;
                const parts = text.split(timestampRegex);
                if (parts.length === 1) return text;
                let html = parts[0];
                for (let i = 1; i < parts.length; i += 2) {
                    const timeStr = parts[i];
                    const content = parts[i + 1] || '';
                    const timeSeconds = parseVttTime(timeStr);
                    html += `<span class="word" data-time="${timeSeconds}">${content}</span>`;
                }
                return html;
            }
            async function initPlayer() {
                player.src = audioDataUri;
                const track = document.createElement('track');
                track.kind = 'captions'; track.label = 'English'; track.srclang = 'en'; track.default = true; track.src = vttDataUri;
                player.appendChild(track);
                track.track.mode = 'showing';
                track.addEventListener('load', () => {
                    const cues = track.track.cues;
                    if (!cues) return;
                    for (let i = 0; i < cues.length; i++) {
                        const cueDiv = document.createElement('div');
                        cueDiv.className = 'transcript-line cursor-pointer text-gray-400 hover:text-gray-900 transition-all p-1.5 rounded balanced-text';
                        cueDiv.dataset.index = i;
                        cueDiv.innerHTML = parseWordTimestamps(cues[i].text);
                        cueDiv.onclick = () => { player.currentTime = cues[i].startTime; player.play(); };
                        transcriptView.appendChild(cueDiv);
                    }
                });
                player.addEventListener('timeupdate', () => {
                    const adjustedTime = player.currentTime + TIME_OFFSET;
                    container.querySelectorAll('.active-word').forEach(el => el.classList.remove('active-word'));
                    const activeLine = container.querySelector('.active-line');
                    if (activeLine) {
                        const words = Array.from(activeLine.querySelectorAll('.word'));
                        let latestPassedTime = -1;
                        words.forEach(word => {
                            const wordTime = parseFloat(word.dataset.time);
                            if (wordTime <= adjustedTime && wordTime > latestPassedTime) { latestPassedTime = wordTime; }
                        });
                        if (latestPassedTime !== -1) {
                            words.forEach(word => {
                                if (parseFloat(word.dataset.time) === latestPassedTime) { word.classList.add('active-word'); }
                            });
                        }
                    }
                });
                track.track.addEventListener('cuechange', function() {
                    const activeCues = this.activeCues;
                    container.querySelectorAll('.transcript-line').forEach(el => { el.classList.remove('active-line'); el.classList.add('text-gray-400'); el.classList.remove('text-gray-900'); });
                    if (activeCues && activeCues.length > 0) {
                        const activeIndex = Array.from(this.cues).indexOf(activeCues[0]);
                        const transcriptLine = transcriptView.querySelector(`[data-index="${activeIndex}"]`);
                        if (transcriptLine) {
                            transcriptLine.classList.add('active-line'); transcriptLine.classList.remove('text-gray-400'); transcriptLine.classList.add('text-gray-900');
                            transcriptLine.scrollIntoView({ behavior: 'smooth', block: 'center' });
                        }
                    }
                });
            }
            initPlayer();
        })();
    </script>
</body>
</html>
"""

# -------------------------------------------------------------------------
# MAIN EVALUATION FUNCTION
# -------------------------------------------------------------------------

def run_speech_evaluation(generation_results, stt_model, project_id, location, output_bucket, credentials, issue_description, gemini_eval_model, render_display=True):
    logger.debug("Starting Main Evaluation Loop")

    # Generate Downscoped Token for Playback
    playback_token = ""
    if render_display:
        playback_token = get_downscoped_token(output_bucket)

    if not generation_results:
        logger.warning("No generation results to evaluate.")
        return [], "No results."

    # Reference Preprocessing
    # Assuming all results share the same text structure (A/B testing)
    first_config = generation_results[0]["voice_config"]
    input_source = first_config.get("INPUT_SOURCE", "TEXT")
    synthesis_text = generation_results[0]["text"]

    raw_text_display = synthesis_text.replace("\\n", "\n\n")
    reference_text_for_analysis = raw_text_display
    turns_metadata = []

    if input_source == "MULTI_SPEAKER_MARKUP":
        reference_text_for_analysis = ""
        lines = raw_text_display.split('\n')
        for line in lines:
            if not line.strip(): continue
            if ":" in line:
                parts = line.split(":", 1)
                spk = parts[0].strip()
                txt = parts[1].strip()
                toks = tokenize_reference(txt)
                turns_metadata.append({"speaker": spk, "num_tokens": len(toks)})
                reference_text_for_analysis += txt + "\n"
            else:
                reference_text_for_analysis += line.strip() + "\n"
                if turns_metadata:
                    toks = tokenize_reference(line.strip())
                    turns_metadata[-1]["num_tokens"] += len(toks)

    stt_tasks = {}
    stt_tasks[stt_model] = []

    for i, gen_result in enumerate(generation_results):
        lang = gen_result["voice_config"]["LOCALE"]
        stt_tasks[stt_model].append({"index": i, "gen_result": gen_result, "lang": lang})

    logger.debug("Executing Batch STT Inference")
    stt_outputs = run_batch_stt_inference(stt_tasks, project_id, output_bucket, credentials)

    logger.debug("Compiling final results")
    eval_client = genai.Client(
        vertexai=True,
        project=project_id,
        location=location,
    )
    eval_results = []

    for gen_idx, gen_result in tqdm(enumerate(generation_results), total=len(generation_results), desc="Compiling Results", leave=False):
        voice_config = gen_result["voice_config"]
        stt_results_list = []

        out = stt_outputs.get((gen_idx, stt_model))
        if out: stt_results_list.append(out["result_data"])

        analysis_data = []
        wer = 1.0
        gemini_result = {}
        audio_metrics = run_audio_analysis(gen_result["audio"], 24000)

        if stt_results_list:
            analysis_data = process_stt_results(stt_results_list, reference_text_for_analysis, eval_client, gemini_eval_model)
            n_ref = sum(1 for t in analysis_data if t['status'] in ['correct', 'missing'])
            n_err = sum(1 for t in analysis_data if t['status'] in ['insertion', 'missing'])
            wer = n_err / n_ref if n_ref > 0 else 0.0

            if render_display:
                if input_source == "MULTI_SPEAKER_MARKUP" and turns_metadata:
                    vtt_output = create_multi_speaker_vtt(analysis_data, turns_metadata)
                else:
                    vtt_output = create_vtt(analysis_data)

                # Upload audio for playback
                blob_name = f"playback/eval_{gen_idx}_{uuid.uuid4()}.wav"
                storage_client = storage.Client(project=project_id)
                bucket = storage_client.bucket(output_bucket)
                blob = bucket.blob(blob_name)
                blob.upload_from_string(gen_result["audio"], content_type="audio/wav")

                audio_uri = f"https://storage.googleapis.com/{output_bucket}/{blob_name}?access_token={playback_token}"

                vtt_b64 = base64.b64encode(vtt_output.encode('utf-8')).decode('utf-8')
                unique_id = f"Inf-{gen_idx}"
                config_name = f"{gen_result['service']} {voice_config.get('VOICE_NAME', '')}"
                display(Markdown(f"### Results for {config_name}"))
                html_code = HTML_TEMPLATE.replace("__TITLE__", f"{unique_id} (WER: {wer:.2f})").replace("__CONFIG_NAME__", config_name).replace("__AUDIO_URI__", audio_uri).replace("__VTT_BASE64__", vtt_b64)
                display(HTML(html_code))

        if analysis_data:
            gemini_result = generate_gemini_analysis(
                client=eval_client, model_name=gemini_eval_model, audio_bytes=gen_result["audio"],
                reference_text=reference_text_for_analysis, analysis_data=analysis_data,
                audio_metrics=audio_metrics, issue_description=issue_description, input_source=input_source
            )
        else:
            gemini_result = {"analysis": "Skipped: No STT analysis data.", "correct": False, "reproduced": False}

        eval_results.append({
            "SERVICE": voice_config.get("SERVICE"), "VOICE_NAME": voice_config.get("VOICE_NAME"),
            "WER": wer, "AUDIO_METRICS": audio_metrics, "ANALYSIS": gemini_result["analysis"],
            "CORRECT": gemini_result["correct"], "REPRODUCED": gemini_result["reproduced"],
            "INFERENCE_INDEX": gen_result.get("inference_index"),
            "INPUT_TEXT": synthesis_text,
            "ANALYSIS_DATA": analysis_data,
            "TURNS_METADATA": turns_metadata,
            "AUDIO_BYTES": gen_result["audio"]
        })

    # Global Summary
    summary_input = []
    for r in eval_results:
        # Create a simplified record for the summary model
        summary_input.append({
            "Configuration": f"{r.get('SERVICE')} - {r.get('VOICE_NAME')}",
            "WER": r.get("WER"),
            "Is Correct": r.get("CORRECT"),
            "Issue Reproduced": r.get("REPRODUCED"),
            "Detailed Analysis": r.get("ANALYSIS")
        })

    prompt = f"""
    You are an expert analyst summarizing a Text-to-Speech evaluation (A/B Test).

    Analyze the provided results and write a **Global Analysis Summary**.

    Structure your response to:
    1. **Differentiate clearly** between the voice configurations tested.
    2. **Highlight pros and cons** for each configuration (e.g. pronunciation accuracy, hallucination issues, audio quality).
    3. **Conclude which configuration is better** and why.

    If all configurations performed perfectly, state that clearly.

    Evaluation Data:
    {json.dumps(summary_input, indent=2)}
    """

    try:
        resp = eval_client.models.generate_content(
            model=gemini_eval_model,
            contents=prompt,
            config=types.GenerateContentConfig(response_mime_type="text/plain")
        )
        global_summary = resp.text
    except Exception as e:
        logger.error(f"Global summary generation failed: {e}")
        global_summary = f"Error generating summary: {e}"

    return eval_results, global_summary

def generate_reproduction_input_text(client, model_name, issue, original_text, reproduced_examples, count):
    prompt = f"""
    You are an expert AI Red Teamer. Your goal is to generate a new input text that reproduces a specific audio issue.
    Issue Description: "{issue}"
    Original Text: "{original_text}"

    We have successfully reproduced this issue with the following texts:
    {json.dumps(reproduced_examples)}

    Generate {count} NEW input text(s) that are likely to trigger the same issue.
    The new text should be diverse but retain the stylistic characteristics (e.g., length, emotion, complexity) of the original and reproduced texts that might be contributing to the issue.
    Respond with a JSON list of strings.
    """
    try:
        resp = client.models.generate_content(
            model=model_name, contents=prompt,
            config=types.GenerateContentConfig(response_mime_type="application/json", response_schema={"type": "ARRAY", "items": {"type": "STRING"}})
        )
        return json.loads(resp.text)
    except Exception as e:
        logger.error(f"Failed to generate reproduction text: {e}")
        return []

# @title Text to audio synthesis

GENERATION_RESULTS = run_speech_synthesis(
    voice_configs=VOICE_CONFIGS,
    project_id=GCP_PROJECT_ID,
    location=LOCATION,
    output_bucket=OUTPUT_BUCKET,
    gemini_api_key=GEMINI_API_KEY
)

# @title Main Evaluation Loop

ORIGINAL_TEXT = VOICE_CONFIGS[0].get("TEXT", "")

# Initial Evaluation
EVAL_RESULTS, global_analysis_summary = run_speech_evaluation(
    generation_results=GENERATION_RESULTS,
    stt_model=STT_TRANSCRIPTION_MODEL,
    project_id=GCP_PROJECT_ID,
    location=LOCATION,
    output_bucket=OUTPUT_BUCKET,
    credentials=credentials,
    issue_description=ISSUE_DESCRIPTION,
    gemini_eval_model=GEMINI_EVALUATION_MODEL,
    render_display=False
)

# Assign Unique IDs to Initial Results
for i, res in enumerate(EVAL_RESULTS):
    res['UNIQUE_ID'] = f"{res['SERVICE']}-{i}-{res.get('INFERENCE_INDEX')}"

# Reproduction Dataset Loop
if REPRODUCTION_DATASET_SIZE > 0:
    logger.info("Starting Reproduction Dataset Generation Loop")
    reproduction_dataset = []
    # Add currently reproduced items if any
    for res in EVAL_RESULTS:
        if res.get('REPRODUCED', False):
            if res['INPUT_TEXT'] not in reproduction_dataset:
                reproduction_dataset.append(res['INPUT_TEXT'])

    attempts = 0
    max_attempts = REPRODUCTION_DATASET_SIZE * 2

    # Track stats for the generation loop
    reproduction_stats = {
        "total_inferences": 0,
        "reproduced_inferences": 0
    }

    # Client for generating new text
    gen_text_client = genai.Client(
        vertexai=True,
        project=GCP_PROJECT_ID,
        location=LOCATION,
    )

    with tqdm(total=REPRODUCTION_DATASET_SIZE, desc="Generating Dataset") as pbar:
        # Initial fill based on initial results
        pbar.update(len(reproduction_dataset))

        while len(reproduction_dataset) < REPRODUCTION_DATASET_SIZE and attempts < max_attempts:
            attempts += 1

            # Generate Candidate Text
            candidates = generate_reproduction_input_text(
                client=gen_text_client,
                model_name=GEMINI_EVALUATION_MODEL,
                issue=ISSUE_DESCRIPTION,
                original_text=ORIGINAL_TEXT,
                reproduced_examples=reproduction_dataset,
                count=1
            )

            if not candidates:
                continue

            candidate_text = candidates[0]
            logger.debug(f"Testing candidate text: {candidate_text[:50]}...")

            # Prepare configs with candidate text
            candidate_voice_configs = copy.deepcopy(VOICE_CONFIGS)
            for vc in candidate_voice_configs:
                vc['TEXT'] = candidate_text

            # Synthesize
            candidate_results = run_speech_synthesis(
                voice_configs=candidate_voice_configs,
                project_id=GCP_PROJECT_ID,
                location=LOCATION,
                output_bucket=OUTPUT_BUCKET,
                gemini_api_key=GEMINI_API_KEY
            )

            # Evaluate (No Render)
            candidate_eval, _ = run_speech_evaluation(
                generation_results=candidate_results,
                stt_model=STT_TRANSCRIPTION_MODEL,
                project_id=GCP_PROJECT_ID,
                location=LOCATION,
                output_bucket=OUTPUT_BUCKET,
                credentials=credentials,
                issue_description=ISSUE_DESCRIPTION,
                gemini_eval_model=GEMINI_EVALUATION_MODEL,
                render_display=False
            )

            # Check for reproduction
            is_reproduced = False
            for i, r in enumerate(candidate_eval):
                r['UNIQUE_ID'] = f"Repro-{attempts}-{r['SERVICE']}-{r.get('INFERENCE_INDEX', i)}"
                r['IS_REPRODUCTION_ATTEMPT'] = True

                reproduction_stats["total_inferences"] += 1
                if r.get('REPRODUCED', False):
                    reproduction_stats["reproduced_inferences"] += 1
                    is_reproduced = True

            if is_reproduced:
                logger.info(f"Reproduced issue with text: {candidate_text[:50]}...")
                if candidate_text not in reproduction_dataset:
                    reproduction_dataset.append(candidate_text)
                    pbar.update(1)
                EVAL_RESULTS.extend(candidate_eval)
            else:
                logger.debug("Issue not reproduced.")

    logger.info(f"Reproduction Dataset Complete. Total Items: {len(reproduction_dataset)}")

    # Update global summary with dataset stats
    if len(reproduction_dataset) > 0:
        freq = 0
        if reproduction_stats["total_inferences"] > 0:
            freq = (reproduction_stats["reproduced_inferences"] / reproduction_stats["total_inferences"]) * 100

        dataset_summary = (
            f"\n\n### Reproduction Dataset Summary\n"
            f"A reproduction dataset was created with {len(reproduction_dataset)} items.\n"
            f"During generation, the issue was reproduced in {reproduction_stats['reproduced_inferences']} "
            f"out of {reproduction_stats['total_inferences']} inferences (Frequency: {freq:.1f}%)."
        )
        global_analysis_summary += dataset_summary

    # Regenerate AI summary if new issues found to ensure the AI summary reflects all findings
    summary_context = [r for r in EVAL_RESULTS if not r.get('CORRECT', False)]
    if summary_context:
        prompt = f"Summarize the following evaluation results (Issues found): {json.dumps(summary_context, default=str)}\n\nExisting Summary Context:\n{global_analysis_summary}"
        try:
            resp = eval_genai_client.models.generate_content(model=GEMINI_EVALUATION_MODEL, contents=prompt, config=types.GenerateContentConfig(response_mime_type="text/plain"))
            global_analysis_summary = resp.text
        except Exception as e:
             logger.warning(f"Failed to update AI summary: {e}")

# Recalculate summary stats for display
all_correct = all(r.get('CORRECT', False) for r in EVAL_RESULTS)
any_reproduced = any(r.get('REPRODUCED', False) for r in EVAL_RESULTS)
reproduced_indices = [r.get('UNIQUE_ID') for r in EVAL_RESULTS if r.get('REPRODUCED')]

# @title Global Evaluation Summary

import ipywidgets as widgets
from IPython.display import display, clear_output, HTML, Markdown
import base64
import uuid
import json
from google.cloud import storage

display(Markdown(f"**All Correct**: {all_correct}"))
display(Markdown(f"**Any Reproduced**: {any_reproduced}"))
display(Markdown(f"**Reproduced Indices**: {reproduced_indices}"))
display(Markdown(f"## Analysis Summary\n\n{global_analysis_summary}"))

# Interactive Results Explorer
display(Markdown("## Detailed Results Explorer"))

dropdown_options = [(res['UNIQUE_ID'], i) for i, res in enumerate(EVAL_RESULTS)]
dropdown = widgets.Dropdown(
    options=dropdown_options,
    description='Select Result:',
    disabled=False,
)

output_area = widgets.Output()

def render_selected_result(change):
    if change['type'] != 'change' or change['name'] != 'value':
        return

    idx = change['new']
    result = EVAL_RESULTS[idx]

    # Retrieve Data
    analysis_data = result.get("ANALYSIS_DATA", [])
    turns_metadata = result.get("TURNS_METADATA", [])
    # Retrieve audio from result directly
    audio_bytes = result.get("AUDIO_BYTES")

    if not audio_bytes:
        with output_area:
            clear_output()
            print("Error: Could not find audio data for this result.")
        return

    # Determine config for header (service + voice name)
    config_name = f"{result['SERVICE']} {result['VOICE_NAME']}"

    # Infer input source from metadata availability or check original configs if needed
    # Logic from run_speech_evaluation: if turns_metadata exists, it was likely Multi-Speaker Markup
    input_source = "TEXT"
    if turns_metadata:
        input_source = "MULTI_SPEAKER_MARKUP"

    with output_area:
        clear_output()

        # 1. Generate VTT
        if input_source == "MULTI_SPEAKER_MARKUP" and turns_metadata:
            vtt_output = create_multi_speaker_vtt(analysis_data, turns_metadata)
        else:
            vtt_output = create_vtt(analysis_data)

        vtt_b64 = base64.b64encode(vtt_output.encode('utf-8')).decode('utf-8')

        # 2. Upload Audio for Playback (if needed)
        # We need to upload it to generate a signed link
        gen_idx = result.get("INFERENCE_INDEX", 0)
        blob_name = f"playback/eval_{gen_idx}_{uuid.uuid4()}.wav"
        storage_client = storage.Client(project=GCP_PROJECT_ID)
        bucket = storage_client.bucket(OUTPUT_BUCKET)
        blob = bucket.blob(blob_name)
        blob.upload_from_string(audio_bytes, content_type="audio/wav")

        # Generate Token
        playback_token = get_downscoped_token(OUTPUT_BUCKET)
        audio_uri = f"https://storage.googleapis.com/{OUTPUT_BUCKET}/{blob_name}?access_token={playback_token}"

        # 3. Render HTML
        unique_id = result['UNIQUE_ID']
        wer = result.get("WER", 0.0)

        # Use HTML_TEMPLATE (Global)
        html_code = HTML_TEMPLATE.replace("__TITLE__", f"{unique_id} (WER: {wer:.2f})") \
                                 .replace("__CONFIG_NAME__", config_name) \
                                 .replace("__AUDIO_URI__", audio_uri) \
                                 .replace("__VTT_BASE64__", vtt_b64)

        display(HTML(html_code))

        # 4. Show Analysis
        display(Markdown(f"### Analysis\n{result.get('ANALYSIS', 'No analysis available')}"))

        # Show Metrics
        metrics = result.get("AUDIO_METRICS", {})
        display(Markdown(f"**Audio Metrics**: {json.dumps(metrics, indent=2)}"))

dropdown.observe(render_selected_result, names='value')

display(dropdown)
display(output_area)

# Trigger for first item if exists
if dropdown_options:
    render_selected_result({'type': 'change', 'name': 'value', 'new': dropdown_options[0][1]})

# @title Download Reproduction Information

import zipfile
import io
import json
import uuid
from google.cloud import storage
from IPython.display import display, Markdown

# Create Zip Bundle
zip_buffer = io.BytesIO()
with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_file:
    # Summary
    if 'global_analysis_summary' in globals():
        zip_file.writestr("summary.md", global_analysis_summary)

    # Voice Configs
    if 'VOICE_CONFIGS' in globals():
        zip_file.writestr("voice_configs.json", json.dumps(VOICE_CONFIGS, indent=2, default=str))

    # Eval Results
    if 'EVAL_RESULTS' in globals():
        zip_file.writestr("eval_results.json", json.dumps(EVAL_RESULTS, indent=2, default=str))

    # Audio
    if 'GENERATION_RESULTS' in globals():
        for res in GENERATION_RESULTS:
            service = res.get("service", "unknown").replace(" ", "_")
            voice_name = res.get("voice_config", {}).get("VOICE_NAME", "unknown")
            idx = res.get("inference_index", 0)
            filename = f"audio/{service}_{voice_name}_{idx}.wav"
            zip_file.writestr(filename, res["audio"])

# Upload
output_filename = f"reproduction_bundle_{uuid.uuid4()}.zip"
bucket_name = OUTPUT_BUCKET
storage_client = storage.Client(project=GCP_PROJECT_ID)
bucket = storage_client.bucket(bucket_name)
blob = bucket.blob(output_filename)

blob.upload_from_string(zip_buffer.getvalue(), content_type="application/zip")
print(f"Uploaded bundle to gs://{bucket_name}/{output_filename}")

# Generate Link
token = get_downscoped_token(bucket_name)
if token:
    download_url = f"https://storage.googleapis.com/{bucket_name}/{output_filename}?access_token={token}"
    display(Markdown(f"### Download Reproduction Bundle"))
    display(Markdown(f"[Download ZIP]({download_url})"))
else:
    print("Could not generate token for download.")