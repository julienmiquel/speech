{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gizeh pydub\n",
    "!pip install moviepy --upgrade\n",
    "!pip install ffmpeg --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get audio samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French great podcast \n",
    "!wget https://audio.audiomeans.fr/file/eiYNOubrDD/65beb744-01fc-40e2-8bb0-312c16855cbb.mp3 \n",
    "!mv *.mp3 ../assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English podcast\n",
    "!gsutil cp gs://github-repo/generative-ai/gemini/long-context/*.mp3 ../assets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the quality of the he audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub.utils import mediainfo\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def change_bitrate(file    ,root_dir = \"../assets/\", output_dir = \"../assets/resampling/\"):\n",
    "    media_info = mediainfo(root_dir+file)\n",
    "    print(media_info)\n",
    "    original_bitrate = media_info['bit_rate']\n",
    "\n",
    "    print(f\"original_bitrate = {original_bitrate}\") \n",
    "    bitrates = [#original_bitrate]# \n",
    "        \"64k\", \"128k\", \"256k\"\n",
    "        #, 312000\n",
    "        ]\n",
    "\n",
    "    frame_rates = [8000, 16000, 22000, 44000, 48000]\n",
    "\n",
    "    frame_rate = \"\"\n",
    "    # Iterate over each audio files\n",
    "    for bitrate in bitrates:\n",
    "        for frame_rate in frame_rates:\n",
    "            sound = AudioSegment.from_mp3(root_dir+file,\n",
    "                                        # 2 byte (16 bit) samples\n",
    "                                        #sample_width=1,\n",
    "                                        # frame rate\n",
    "                                        frame_rate=frame_rate,\n",
    "                                        # Mono\n",
    "                                        channels=1\n",
    "                                        )\n",
    "            \n",
    "\n",
    "            print(80*\"__\")\n",
    "            print(f\"file = {file}\")\n",
    "\n",
    "            print(f\"duration_seconds = {sound.duration_seconds}\")\n",
    "            print(f\"sample_width = {sound.sample_width}\")\n",
    "            print(f\"channels = {sound.channels}\")\n",
    "            print(f\"frame_rate = {sound.frame_rate}\")\n",
    "            sound = sound.set_frame_rate(frame_rate)\n",
    "            print(f\"frame_rate updated = {sound.frame_rate}\")\n",
    "\n",
    "            sound.export(output_dir+f\"{file}-{bitrate}-{frame_rate}.mp3\" , format=\"mp3\"\n",
    "                        , bitrate=bitrate\n",
    "                        )\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"65beb744-01fc-40e2-8bb0-312c16855cbb.mp3\"\n",
    "change_bitrate(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the format ==> Convert to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioedit import combine_audio_into_video\n",
    "from moviepy.editor import VideoFileClip, TextClip, ColorClip\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "from pydub.utils import mediainfo\n",
    "\n",
    "def convert_to_wav_16k(audio_path):\n",
    "    output_filename = audio_path.replace(\".mp3\", \".wav\") \n",
    "    AudioSegment.from_mp3(audio_path).export(output_filename, format=\"wav\", parameters=[\"-ar\", \"16000\"])\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def convert_to_video(audio_path):\n",
    "    output_filename = audio_path.replace(\".mp3\", \".mp4\") \n",
    "    #output_filename = audio_path + \".mp4\"\n",
    "    # original_bitrate = mediainfo(audio_path)['bit_rate']\n",
    "    # print(\"original_bitrate:\",original_bitrate)\n",
    "\n",
    "    # print(mediainfo(audio_path))\n",
    "    \n",
    "    # Load audio clips\n",
    "    audio_clips = AudioFileClip(audio_path)\n",
    "    total_audio = audio_clips.duration \n",
    "    print(f\"total_audio = {total_audio}\")\n",
    "    video_clip = ColorClip(size=(1,1), color=(255,255,255) #, duration=total_audio\n",
    "                           )\n",
    "\n",
    "    # Combine and save the video\n",
    "    video_clip = video_clip.set_audio(audio_clips).set_duration(total_audio)\n",
    "    \n",
    "    #video_clip.set_duration(total_audio)\n",
    "    video_clip.write_videofile(\n",
    "        filename=output_filename, fps=1, codec=\"libx264\",\n",
    "                        bitrate=\"1k\", \n",
    "                        audio=True, audio_fps=16000,\n",
    "                        preset=\"medium\",\n",
    "                        audio_nbytes=4, audio_codec=\"libmp3lame\",\n",
    "                        audio_bitrate=\"128k\", audio_bufsize=2000,\n",
    "                        #temp_audiofile=None,\n",
    "                        rewrite_audio=False, remove_temp=True,\n",
    "                        write_logfile=False, verbose=True,\n",
    "                        #threads=None, \n",
    "                        #ffmpeg_params=None\n",
    "    )\n",
    "\n",
    "    return output_filename\n",
    "        # output_filename, #codec=\"libx264\",\n",
    "        #                                                   audio = True, fps=1)\n",
    "\n",
    "file_name = \"/Users/julienmiquel/dev/STT/assets/Interview-Stephane-Gachet-JO.mp3\"\n",
    "\n",
    "\n",
    "#convert_to_video(file_name)    \n",
    "convert_to_wav_16k(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs.write_file_to_gcs(config.BUCKET,f\"stt/Interview-Stephane-Gachet-JO.mp4\",\"/Users/julienmiquel/dev/STT/assets/Interview-Stephane-Gachet-JO.mp4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "root_dir = \"/Users/julienmiquel/dev/STT/stt/\"\n",
    "# Get a list of all TSV files in the current folder\n",
    "mp3_files = glob.glob(root_dir=root_dir, pathname=\n",
    "                    \"*.mp3\")                      \n",
    "\n",
    "wer_results = {}\n",
    "# Iterate over each prompt file\n",
    "for mp3_file in mp3_files:\n",
    "    convert_to_video(root_dir+mp3_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install sequence-evaluate rouge bleu transformers sentence-transformers torch rouge numpy seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from seq_eval import SeqEval\n",
    "\n",
    "evaluator = SeqEval()\n",
    "\n",
    "root_dir = \"/Users/julienmiquel/dev/STT/tests/\"\n",
    "# Get a list of all TSV files in the current folder\n",
    "tsv_files = glob.glob(root_dir=root_dir, pathname=\n",
    "                      \"4_short_*.tsv\")\n",
    "                      #\"2_long*0.tsv\")\n",
    "\n",
    "file_ref = \"4_short2_gemini-1.5-pro-001_0.tsv\"\n",
    "file_candidate = \"4_short_gemini-1.5-pro-001_0.tsv\"\n",
    "\n",
    "# Read the TSV file into a dataframe\n",
    "df_ref = pd.read_csv(root_dir+file_ref, sep='\\t')\n",
    "df_candidate = pd.read_csv(root_dir+file_candidate, sep='\\t')\n",
    "    \n",
    "column_analyze = \"text\" # \"speakerid\" #\n",
    "references = df_ref[column_analyze].to_list()\n",
    "candidates = df_candidate[column_analyze].to_list()\n",
    "embed_ref = embed_text(references)\n",
    "embed_candidate = embed_text(candidates)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluator.evaluate(candidates, references, verbose=True)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_sim_array = cosine_similarity(X=embed_ref, Y=embed_candidate)\n",
    "\n",
    "# display as DataFrame\n",
    "axe = 'name'\n",
    "# apply lambda to each row of the references list to get the first 5 letters of each string in the list\n",
    "references_shorten = pd.DataFrame({'name': references})['name'].apply(lambda x: str(x)[0:100]).to_list()\n",
    "candidates_shorten = pd.DataFrame({'name': candidates})['name'].apply(lambda x: str(x)[0:100]).to_list()\n",
    "\n",
    "#df_sim = pd.DataFrame(cos_sim_array, index=references, columns=candidates)\n",
    "df_sim = pd.DataFrame(cos_sim_array, index=references, columns=candidates)\n",
    "\n",
    "print(f\"columns = {len(df_sim.columns)}\")\n",
    "print(f\"rows = {len(df_sim.index)}\")\n",
    "\n",
    "while len(df_sim.columns) < len(df_sim.values):\n",
    "    id = df_sim.max(axis=1).idxmin() #.loc( df_sim.idxmax( axis='columns')) #.idxmin(axis='columns') #.apply(lambda x: np.max(x), axis='rows')\n",
    "    print(f\"{id}  =  {df_sim[id].max()}\")\n",
    "    print(f\"drop id = {id}\")\n",
    "    df_sim = df_sim.columns.drop(id)\n",
    "    print(f\"columns = {len(df_sim.columns)}\")\n",
    "    print(f\"rows = {len(df_sim.index)}\")    \n",
    "    \n",
    "\n",
    "while len(df_sim.columns) > len(df_sim.values):\n",
    "    id = df_sim.max(axis=0).idxmin() #.loc( df_sim.idxmax( axis='columns')) #.idxmin(axis='columns') #.apply(lambda x: np.max(x), axis='rows')\n",
    "    print(f\"drop id = {id}\")\n",
    "    df_sim.drop(index=id)\n",
    "    print(f\"columns = {len(df_sim.columns)}\")\n",
    "    print(f\"rows = {len(df_sim.index)}\")    \n",
    "\n",
    "#df_sim#.idxmax( axis='columns')\n",
    "\n",
    "\n",
    "#df_sim\n",
    "# filter value inferior to 0.95\n",
    "#df_sim.loc[ (df_sim['max'] < 0.75)]\n",
    "\n",
    "#df_sim#['max'].idxmin( )\n",
    "#df_sim['max'].filter( ,axis = 'index').idxmin()\n",
    "# df_max = pd.DataFrame(df_sim.apply(lambda x: np.max(x), axis='rows'))\n",
    "# df_max.columns = [\"value\"]\n",
    "#df_max.apply(lambda x: np.min(x), axis='columns')\n",
    "\n",
    "#df_sim['max'].min() #[df_max[\"value\"] == df_max[\"max\"].min()] #.apply(lambda x: np.min(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(df_sim,\n",
    "                 #annot=True, \n",
    "                 cmap=\"crest\")\n",
    "#ax.xaxis.tick_top()\n",
    "ax.set_xticklabels(candidates_shorten, rotation=90)\n",
    "ax.set_yticklabels(references_shorten)\n",
    "ax.set_title(f'Cosine Similarity for {column_analyze} (text are truncated)'  )\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_array = np.array(embed_ref, dtype=np.float32)\n",
    "# tsne = TSNE(random_state=0, n_iter=250)\n",
    "# tsne_results = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# df_tsne = pd.DataFrame(tsne_results, columns=[\"TSNE1\", \"TSNE2\"])\n",
    "# df_tsne[\"target\"] = df[\"target\"]  # Add labels column from df_train to df_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "\n",
    "\n",
    "def embed_text(\n",
    "    texts: List[str] ,\n",
    "    task: str = \"RETRIEVAL_DOCUMENT\",\n",
    "    model_name: str = \"text-embedding-004\",\n",
    "    dimensionality: Optional[int] = 256,\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"Embeds texts with a pre-trained, foundational model.\"\"\"\n",
    "    model = TextEmbeddingModel.from_pretrained(model_name)\n",
    "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
    "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
    "    embeddings = model.get_embeddings(inputs, **kwargs)\n",
    "    return [embedding.values for embedding in embeddings]\n",
    "\n",
    "#embed_text([\"banana muffins? \", \"banana bread? banana muffins?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_file =  json.loads(open(\"./whisper/whisper_UGC - meeting.mp3_medium.json\").read())\n",
    "df_whisper = pd.DataFrame(whisper_file[\"segments\"])\n",
    "df_whisper.drop(columns=[\"id\",\"seek\",\"tokens\",\"temperature\",\"avg_logprob\",\"compression_ratio\",\"no_speech_prob\"], inplace = True)\n",
    "\n",
    "df_whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "root_dir = \"/Users/julienmiquel/dev/STT/tests/\"\n",
    "# Get a list of all TSV files in the current folder\n",
    "tsv_files = glob.glob(root_dir=root_dir, pathname=\n",
    "                      \"4_short_*.tsv\")\n",
    "                      #\"2_long*0.tsv\")\n",
    "\n",
    "if len(tsv_files) > 0:\n",
    "    print(\"Found TSV files\")\n",
    "    # Create an empty list to store the dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate over each TSV file\n",
    "    for file in tsv_files:\n",
    "        # Read the TSV file into a dataframe\n",
    "        df = pd.read_csv(root_dir+file, sep='\\t')\n",
    "        df['file'] = file\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all dataframes into a single dataframe\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Now you have a single dataframe 'combined_df' containing data from all TSV files\n",
    "    print(combined_df)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo_json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "root_dir = \"/Users/julienmiquel/dev/STT/assets/\"\n",
    "output_dir = \"/Users/julienmiquel/dev/STT/assets/resampling/\"\n",
    "# Get a list of all TSV files in the current folder\n",
    "mp3_files = glob.glob(root_dir=root_dir, pathname=\n",
    "                      \"*.mp3\")\n",
    "                      #\"2_long*0.tsv\")\n",
    "bitrates = [64, 92, 128, 256, 312]\n",
    "\n",
    "# Iterate over each TSV file\n",
    "for file in mp3_files:\n",
    "    prompt = \"\"\"Chapterize the audio content by grouping the audio content into chapters by detecting silence. \n",
    "Please only capture key events and highlights. If you are not sure about any info, please do not make it up. \n",
    "Return the result in the JSON format with keys as follows : 'timecode_start', 'timecode_stop'\"\"\"\n",
    "\n",
    "    sound = AudioSegment.from_mp3(root_dir+file)\n",
    "    print(80*\"__\")\n",
    "    print(f\"file = {file}\")\n",
    "\n",
    "    print(f\"duration_seconds = {sound.duration_seconds}\")\n",
    "    print(f\"sample_width = {sound.sample_width}\")\n",
    "    print(f\"channels = {sound.channels}\")\n",
    "    print(f\"frame_rate = {sound.frame_rate}\")\n",
    "    \n",
    "    sound.export(output_dir+file+f\"-{bitrate}.mp3\", format=\"mp3\", bitrate=bitrate)\n",
    "    break\n",
    "    \n",
    "\n",
    "#print(f\"duration = {sound.duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/Users/julienmiquel/dev/STT/assets/\"\n",
    "output_dir = \"/Users/julienmiquel/dev/STT/assets/assets_splitted/\"\n",
    "\n",
    "\n",
    "def splitAudio(root_dir, file, start, stop, output_dir):\n",
    "    sound = AudioSegment.from_mp3(root_dir+file)\n",
    "    print(80*\"__\")\n",
    "    print(f\"file = {file}\")\n",
    "\n",
    "    sound = sound[start:stop]\n",
    "    print(f\"duration_seconds = {sound.duration_seconds}\")\n",
    "    print(f\"sample_width = {sound.sample_width}\")\n",
    "    print(f\"channels = {sound.channels}\")\n",
    "    print(f\"frame_rate = {sound.frame_rate}\")\n",
    "    file_segment = output_dir+file+f\"-{start}-{stop}.mp3\"\n",
    "\n",
    "    sound.export(file_segment, format=\"mp3\")\n",
    "    return file_segment\n",
    "\n",
    "splitAudio(root_dir, \"Lionel 1.mp3\", 10, 15,output_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gemini = GenerativeModel(model_name)\n",
    "\n",
    "# Iterate over each mp3 file\n",
    "file = \"UGC_meeting.mp3\"\n",
    "file = \"Lionel 1.mp3\"\n",
    "\n",
    "audio1 = Part.from_uri(\n",
    "    mime_type=\"audio/mp3\",\n",
    "    uri=f\"gs://ml-demo-eu/datasets/input/{file}\")\n",
    "\n",
    "# prompt_split = \"\"\"Chapterize the audio content by grouping the audio content into chapters by detecting silence. \n",
    "# Please only capture key events and highlights. If you are not sure about any info, please do not make it up. \n",
    "# Return the result in the JSON format with keys as follows : 'timecode_start', 'timecode_stop' 'description'\n",
    "# \"\"\"\n",
    "\n",
    "prompt = \"\"\"<Task>\n",
    "    Transcribe in {language} this recording from the time_start 0 to the time_stop 500 of the recording.\n",
    "    Identify each persons with their name and their genre if possible.\n",
    "    Accuracy: Prioritize precision in capturing spoken words. Strive to minimize errors and misinterpretations.\n",
    "    Clarity: Ensure the transcribed text flows naturally and is easy to understand. Punctuate sentences appropriately.\n",
    "\n",
    "    STOP generating output properly (keep a valid JSON) after generating 8000 characters in output.\n",
    "</Task>\n",
    "\n",
    "<answer_format>\n",
    "    JSON keys: id (speaker id), start (time_start), stop (time_stop), text.\n",
    "    Id is numeric id of the speaker (speaker_id). Check twice the speaker id is correctly associated with the text.\n",
    "    start (time_start) and stop (time_stop) are the full time in miliseconds and not truncated.\n",
    "    text is the transcription of the voice of the speaker.\n",
    "</answer_format>\n",
    "\"\"\"\n",
    "\n",
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"time_start\": {\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            \"time_stop\": {\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            \"id\": {\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"STRING\"\n",
    "            }\n",
    "        },\n",
    "        # \"required\": [\n",
    "        #     \"timecode_start\"\n",
    "        # ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# generation_config_json = {\n",
    "#       \"max_output_tokens\": 8192,\n",
    "#       \"temperature\": 0.0,\n",
    "#       \"top_p\": 1,\n",
    "#       \"response_mime_type\": \"application/json\",\n",
    "#       \"response_schema\" : response_schema\n",
    "#       #\"enableStandaloneAudioTimestampsToModels\": True,\n",
    "      \n",
    "#   }\n",
    "\n",
    "generation_config_json = generation_config=GenerationConfig(\n",
    "    max_output_tokens= 8192,\n",
    "    temperature= 0.0,\n",
    "    top_p= 0.0,\n",
    "    frequency_penalty = 0.5,\n",
    "    response_mime_type=\"application/json\", \n",
    "    response_schema=response_schema\n",
    "    )\n",
    "\n",
    "prompts = [audio1, prompt]\n",
    "response = model_gemini.generate_content(\n",
    "    contents=prompts,\n",
    "    generation_config=generation_config_json,\n",
    "    #safety_settings=safety_settings,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "audio_split = json.load(response.text)\n",
    "audio_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_sequence(file):\n",
    "    from pydub import AudioSegment, silence\n",
    "\n",
    "    myaudio = AudioSegment.from_mp3(file)\n",
    "    dBFS=myaudio.dBFS\n",
    "\n",
    "    speak = silence.detect_nonsilent(myaudio, min_silence_len=500, silence_thresh=dBFS-20, seek_step=10)\n",
    "    speak_sequences = speak\n",
    "\n",
    "    #speak_sequences = [((start/1000),(stop/1000)) for start,stop in speak] #convert to sec\n",
    "\n",
    "    print(speak_sequences)\n",
    "    return speak_sequences\n",
    "\n",
    "speak_sequences = get_audio_sequence(root_dir+file)\n",
    "\n",
    "# filter speak_sequences when stop - start are less than 2 secondes\n",
    "speak_sequences_filtered = [(start, stop) for start, stop in speak_sequences if stop - start >= 1500]\n",
    "\n",
    "model_gemini = GenerativeModel(model_name)\n",
    "\n",
    "prompt = \"\"\"<Task>\n",
    "    Transcribe in {language} this recording from the time_start 0 to the time_stop 500 of the recording.\n",
    "    Identify each persons with their name and their genre if possible.\n",
    "    Accuracy: Prioritize precision in capturing spoken words. Strive to minimize errors and misinterpretations.\n",
    "    Clarity: Ensure the transcribed text flows naturally and is easy to understand. Punctuate sentences appropriately.\n",
    "\n",
    "    STOP generating output properly (keep a valid JSON) after generating 8000 characters in output.\n",
    "</Task>\n",
    "\n",
    "<answer_format>\n",
    "    JSON keys: id (speaker id), start (time_start), stop (time_stop), text.\n",
    "    Id is numeric id of the speaker (speaker_id). Check twice the speaker id is correctly associated with the text.\n",
    "    start (time_start) and stop (time_stop) are the full time in seconds and not truncated.\n",
    "    text is the transcription of the voice of the speaker.\n",
    "</answer_format>\n",
    "\"\"\"\n",
    "splited_files = []\n",
    "splited_answers = []\n",
    "for (start, stop) in speak_sequences_filtered:\n",
    "    \n",
    "    splited_file = splitAudio(root_dir,file, start, stop, output_dir)\n",
    "\n",
    "    # Load file bytes\n",
    "    with open(splited_file, \"rb\") as f:\n",
    "        file_part = Part.from_data(data=f.read(), mime_type=\"audio/mp3\")\n",
    "\n",
    "    print(splited_file)\n",
    "    splited_files.append(splited_file)\n",
    "\n",
    "    # Load contents\n",
    "    contents = [file_part, prompt]\n",
    "\n",
    "    # Send to Gemini with GenerationConfig\n",
    "    response = model_gemini.generate_content(contents, generation_config=generation_config_json)\n",
    "    print(response.text)\n",
    "    splited_answers.append(response.text)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Speech-to-Text client library\n",
    "from google.cloud import speech\n",
    "\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient() \n",
    "\n",
    "audio_content = None\n",
    "\n",
    "# transcribe speech\n",
    "audio = speech.RecognitionAudio(content=audio_content)\n",
    "\n",
    "diarizationConfig = speech.SpeakerDiarizationConfig(enable_speaker_diarization=True)\n",
    "\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    sample_rate_hertz=24000,\n",
    "    language_code=\"fr-FR\",\n",
    "    model=\"default\",\n",
    "    audio_channel_count=1,\n",
    "    enable_word_confidence=True,\n",
    "    enable_word_time_offsets=True,\n",
    "    diarization_config = diarizationConfig,\n",
    ")\n",
    "\n",
    "# Detects speech in the audio file\n",
    "operation = client.long_running_recognize(config=config, audio=audio)\n",
    "\n",
    "print(\"Waiting for operation to complete...\")\n",
    "response = operation.result(timeout=90)\n",
    "\n",
    "for result in response.results:\n",
    "  print(\"Transcript: {}\".format(result.alternatives[0].transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_streaming(stream_file: str) -> speech.RecognitionConfig:\n",
    "    \"\"\"Streams transcription of the given audio file.\"\"\"\n",
    "\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    with open(stream_file, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "\n",
    "    # In practice, stream should be a generator yielding chunks of audio data.\n",
    "    stream = [content]\n",
    "\n",
    "    requests = (\n",
    "        speech.StreamingRecognizeRequest(audio_content=chunk) for chunk in stream\n",
    "    )\n",
    "\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "\n",
    "    streaming_config = speech.StreamingRecognitionConfig(config=config)\n",
    "\n",
    "    # streaming_recognize returns a generator.\n",
    "    responses = client.streaming_recognize(\n",
    "        config=streaming_config,\n",
    "        requests=requests,\n",
    "    )\n",
    "\n",
    "    for response in responses:\n",
    "        # Once the transcription has settled, the first result will contain the\n",
    "        # is_final result. The other results will be for subsequent portions of\n",
    "        # the audio.\n",
    "        for result in response.results:\n",
    "            print(f\"Finished: {result.is_final}\")\n",
    "            print(f\"Stability: {result.stability}\")\n",
    "            alternatives = result.alternatives\n",
    "            # The alternatives are ordered from most likely to least.\n",
    "            for alternative in alternatives:\n",
    "                print(f\"Confidence: {alternative.confidence}\")\n",
    "                print(f\"Transcript: {alternative.transcript}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepgram-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepgram import DeepgramClient, PrerecordedOptions\n",
    "\n",
    "# The API key we created in step 3\n",
    "DEEPGRAM_API_KEY = 'ac4855260ef2e43fb49307f2e58e69aee84e8f96'\n",
    "\n",
    "# Replace with your file path\n",
    "PATH_TO_FILE = root_dir + \"Lionel 1.mp3\"\n",
    "\n",
    "def main():\n",
    "    deepgram = DeepgramClient(DEEPGRAM_API_KEY)\n",
    "\n",
    "    with open(PATH_TO_FILE, 'rb') as buffer_data:\n",
    "        payload = { 'buffer': buffer_data }\n",
    "\n",
    "        options = PrerecordedOptions(\n",
    "            punctuate=True, model=\"nova-2\", language=\"fr\"\n",
    "        )\n",
    "\n",
    "        response = deepgram.listen.prerecorded.v('1').transcribe_file(payload, options)\n",
    "        print(response.to_json(indent=4))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
